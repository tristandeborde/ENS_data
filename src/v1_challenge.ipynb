{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import datetime\n",
    "import umap\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import keras\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tsfresh import extract_features\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Activation, Dense, concatenate, LSTM, GRU, Dropout\n",
    "from sklearn import metrics, feature_selection\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from random import uniform, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    dfx = pd.read_csv('../data/x_train.csv').set_index('ID')\n",
    "    dfy = pd.read_csv('../data/y_train.csv').set_index('ID')\n",
    "    dfx_test = pd.read_csv('../data/x_test.csv').set_index('ID')\n",
    "    return dfx, dfy, dfx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron_id</th>\n",
       "      <th>timestamp_0</th>\n",
       "      <th>timestamp_1</th>\n",
       "      <th>timestamp_2</th>\n",
       "      <th>timestamp_3</th>\n",
       "      <th>timestamp_4</th>\n",
       "      <th>timestamp_5</th>\n",
       "      <th>timestamp_6</th>\n",
       "      <th>timestamp_7</th>\n",
       "      <th>timestamp_8</th>\n",
       "      <th>...</th>\n",
       "      <th>timestamp_40</th>\n",
       "      <th>timestamp_41</th>\n",
       "      <th>timestamp_42</th>\n",
       "      <th>timestamp_43</th>\n",
       "      <th>timestamp_44</th>\n",
       "      <th>timestamp_45</th>\n",
       "      <th>timestamp_46</th>\n",
       "      <th>timestamp_47</th>\n",
       "      <th>timestamp_48</th>\n",
       "      <th>timestamp_49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>5691</td>\n",
       "      <td>0.077914</td>\n",
       "      <td>0.708334</td>\n",
       "      <td>1.009554</td>\n",
       "      <td>1.125147</td>\n",
       "      <td>1.271336</td>\n",
       "      <td>1.299890</td>\n",
       "      <td>1.666290</td>\n",
       "      <td>1.718390</td>\n",
       "      <td>2.381562</td>\n",
       "      <td>...</td>\n",
       "      <td>29.316422</td>\n",
       "      <td>30.307006</td>\n",
       "      <td>31.185741</td>\n",
       "      <td>31.227892</td>\n",
       "      <td>32.320902</td>\n",
       "      <td>32.701000</td>\n",
       "      <td>32.955075</td>\n",
       "      <td>33.016627</td>\n",
       "      <td>34.837705</td>\n",
       "      <td>34.874491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16631</th>\n",
       "      <td>2341</td>\n",
       "      <td>0.485287</td>\n",
       "      <td>0.870193</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>1.733133</td>\n",
       "      <td>1.755243</td>\n",
       "      <td>1.803468</td>\n",
       "      <td>1.841432</td>\n",
       "      <td>1.986925</td>\n",
       "      <td>2.006145</td>\n",
       "      <td>...</td>\n",
       "      <td>17.151013</td>\n",
       "      <td>17.367892</td>\n",
       "      <td>17.727558</td>\n",
       "      <td>18.178916</td>\n",
       "      <td>18.521734</td>\n",
       "      <td>19.492522</td>\n",
       "      <td>19.515122</td>\n",
       "      <td>20.715555</td>\n",
       "      <td>21.217199</td>\n",
       "      <td>21.640693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16632</th>\n",
       "      <td>8046</td>\n",
       "      <td>0.213619</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>1.575419</td>\n",
       "      <td>1.650658</td>\n",
       "      <td>1.700773</td>\n",
       "      <td>1.856047</td>\n",
       "      <td>1.927563</td>\n",
       "      <td>1.950001</td>\n",
       "      <td>2.367852</td>\n",
       "      <td>...</td>\n",
       "      <td>14.064862</td>\n",
       "      <td>14.092407</td>\n",
       "      <td>14.343008</td>\n",
       "      <td>14.428562</td>\n",
       "      <td>14.671081</td>\n",
       "      <td>14.791297</td>\n",
       "      <td>14.847738</td>\n",
       "      <td>14.916361</td>\n",
       "      <td>15.055357</td>\n",
       "      <td>15.192531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16633</th>\n",
       "      <td>6855</td>\n",
       "      <td>2.669642</td>\n",
       "      <td>7.500198</td>\n",
       "      <td>8.710341</td>\n",
       "      <td>8.724346</td>\n",
       "      <td>8.760471</td>\n",
       "      <td>8.770804</td>\n",
       "      <td>8.786001</td>\n",
       "      <td>8.847625</td>\n",
       "      <td>8.885186</td>\n",
       "      <td>...</td>\n",
       "      <td>15.555430</td>\n",
       "      <td>15.698512</td>\n",
       "      <td>15.782122</td>\n",
       "      <td>16.067267</td>\n",
       "      <td>16.499324</td>\n",
       "      <td>16.906955</td>\n",
       "      <td>17.444176</td>\n",
       "      <td>18.704728</td>\n",
       "      <td>22.082864</td>\n",
       "      <td>27.185064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16634</th>\n",
       "      <td>6921</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>0.376280</td>\n",
       "      <td>0.445379</td>\n",
       "      <td>0.762095</td>\n",
       "      <td>0.929728</td>\n",
       "      <td>1.029945</td>\n",
       "      <td>1.622643</td>\n",
       "      <td>1.710542</td>\n",
       "      <td>2.046901</td>\n",
       "      <td>...</td>\n",
       "      <td>23.397511</td>\n",
       "      <td>23.437452</td>\n",
       "      <td>24.217391</td>\n",
       "      <td>24.519848</td>\n",
       "      <td>24.675594</td>\n",
       "      <td>24.764713</td>\n",
       "      <td>24.894686</td>\n",
       "      <td>25.041824</td>\n",
       "      <td>25.071265</td>\n",
       "      <td>28.288743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       neuron_id  timestamp_0  timestamp_1  timestamp_2  timestamp_3  \\\n",
       "ID                                                                     \n",
       "16630       5691     0.077914     0.708334     1.009554     1.125147   \n",
       "16631       2341     0.485287     0.870193     0.959064     1.733133   \n",
       "16632       8046     0.213619     0.290771     1.575419     1.650658   \n",
       "16633       6855     2.669642     7.500198     8.710341     8.724346   \n",
       "16634       6921     0.075924     0.376280     0.445379     0.762095   \n",
       "\n",
       "       timestamp_4  timestamp_5  timestamp_6  timestamp_7  timestamp_8  ...  \\\n",
       "ID                                                                      ...   \n",
       "16630     1.271336     1.299890     1.666290     1.718390     2.381562  ...   \n",
       "16631     1.755243     1.803468     1.841432     1.986925     2.006145  ...   \n",
       "16632     1.700773     1.856047     1.927563     1.950001     2.367852  ...   \n",
       "16633     8.760471     8.770804     8.786001     8.847625     8.885186  ...   \n",
       "16634     0.929728     1.029945     1.622643     1.710542     2.046901  ...   \n",
       "\n",
       "       timestamp_40  timestamp_41  timestamp_42  timestamp_43  timestamp_44  \\\n",
       "ID                                                                            \n",
       "16630     29.316422     30.307006     31.185741     31.227892     32.320902   \n",
       "16631     17.151013     17.367892     17.727558     18.178916     18.521734   \n",
       "16632     14.064862     14.092407     14.343008     14.428562     14.671081   \n",
       "16633     15.555430     15.698512     15.782122     16.067267     16.499324   \n",
       "16634     23.397511     23.437452     24.217391     24.519848     24.675594   \n",
       "\n",
       "       timestamp_45  timestamp_46  timestamp_47  timestamp_48  timestamp_49  \n",
       "ID                                                                           \n",
       "16630     32.701000     32.955075     33.016627     34.837705     34.874491  \n",
       "16631     19.492522     19.515122     20.715555     21.217199     21.640693  \n",
       "16632     14.791297     14.847738     14.916361     15.055357     15.192531  \n",
       "16633     16.906955     17.444176     18.704728     22.082864     27.185064  \n",
       "16634     24.764713     24.894686     25.041824     25.071265     28.288743  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx, dfy, dfx_test = import_data()\n",
    "dfy.head()\n",
    "dfx.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same number of samples, all good.\n"
     ]
    }
   ],
   "source": [
    "if dfy.shape[0] == dfx.shape[0]:\n",
    "    print(\"Same number of samples, all good.\")\n",
    "else:\n",
    "    print(\"Different number of samples, problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sanity check:** diff etat1/etat2, neuron_id usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DiffÃ©rence entre le nombre d'etats 1 et d'etats 0.\n",
    "dfy.TARGET.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 neuron_ids in common between the train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Should we keep the neuron_id col ?\n",
    "xtest_uniques = dfx_test.neuron_id.unique()\n",
    "x_uniques = dfx.neuron_id.unique()\n",
    "diff = [x for x in x_uniques if x in xtest_uniques]\n",
    "print(\"There are {} neuron_ids in common between the train and test sets.\".format(len(diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Balance dataset:** Choose between undersampling and oversampling, to get equal nb of samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X, y, method=\"undersampling\", **params):\n",
    "    \"\"\" Return balanced training dataset obtained by undersampling class 2. \"\"\"\n",
    "    if method == \"oversampling\":\n",
    "        sampler = RandomOverSampler(random_state=42)\n",
    "    elif method == \"undersampling\":\n",
    "        sampler = RandomUnderSampler(random_state=42)\n",
    "    else:\n",
    "        raise ValueError('Unrecognized sampling method: ', method)\n",
    "\n",
    "    X, y = sampler.fit_resample(X, y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Extract features:** Use tsfresh to perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tsfresh(X):\n",
    "    # Create a df with integer names for columns so as to facilitate sorting\n",
    "    cols = [col for col in range(1, 50)]\n",
    "    neuron_ids = X[:, 0]\n",
    "    ts_df = pd.DataFrame(X[:, 1:], columns=cols)\n",
    "    \n",
    "    # Stack the df to conform to tsfresh format\n",
    "    ts_df = pd.DataFrame(ts_df.stack()).reset_index()\n",
    "    ts_df.columns = [\"ID\", \"timestamp\", \"spike_time\"]\n",
    "    fe_arr = extract_features(ts_df, column_id=\"ID\", column_value=\"spike_time\", column_sort=\"timestamp\")\n",
    "    \n",
    "    # Concatenate into an array, the engineered features df and the time series df\n",
    "    ts_df = ts_df.pivot(index='ID', columns='timestamp', values='spike_time')\n",
    "    ts_arr = ts_df.values\n",
    "    fe_arr = fe_arr.dropna(axis='columns')\n",
    "    fe_arr = fe_arr.values\n",
    "    X = np.concatenate((neuron_ids, ts_arr, fe_arr), axis=1)\n",
    "    \n",
    "    # Save extracted features\n",
    "    fname = \"../data/features{}.csv\".format(datetime.datetime.now().strftime(\"%m%d%H%M%S\"))\n",
    "    np.savetxt(fname, X, delimiter=\",\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_manual(X):\n",
    "    neuron_ids = X[:, 0]\n",
    "    X = X[:, 1:]\n",
    "    features = np.concatenate((\n",
    "        np.mean(X, axis=1)[..., np.newaxis],\n",
    "        np.amin(X, axis=1)[..., np.newaxis],\n",
    "        np.amax(X, axis=1)[..., np.newaxis]\n",
    "    ), axis=1)\n",
    "    quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    for q in quantiles:\n",
    "        features = np.concatenate((features, np.quantile(X, q, axis=1)[..., np.newaxis]), axis=1)\n",
    "    X = np.concatenate((neuron_ids[..., np.newaxis], X, features), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_features(X):\n",
    "    ts_arr = X[:,1:51]\n",
    "    fe_arr = np.concatenate((X[:,0][..., np.newaxis], X[:,51:]), axis=1)\n",
    "    return [ts_arr, fe_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Feature engineering 2.0:** Add sums from original ts, and groupby neuron_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sommes(X, step):\n",
    "    \"\"\"\n",
    "    dfx, dfy, dfx_test = import_data()\n",
    "    if step == \"train\":\n",
    "        df = dfx\n",
    "    else:\n",
    "        df = dfx_test\n",
    "    \"\"\"    \n",
    "    somme_tokeep = [10, 20, 30, 40, 50]\n",
    "    somme = X[:,np.array(somme_tokeep)]\n",
    "    X = np.concatenate((X, somme), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_neurons(fe_arr):\n",
    "    # Convert fe_arr to DF\n",
    "    columns = [i for i in range(fe_arr.shape[1])]\n",
    "    columns[0] = 'neuron_id'\n",
    "    fe_arr_df = pd.DataFrame(data=fe_arr, columns=columns)\n",
    "    \n",
    "    # Groupby neuron_id, compute mean, drop useless neuron_id col, merge df\n",
    "    neurons_features = fe_arr_df.groupby(['neuron_id']).mean()\n",
    "    final = fe_arr_df.reset_index().merge(neurons_features, on='neuron_id', sort=False).sort_values('index')\n",
    "    final = final.values\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Remove features with low variance:** pretty self-explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_var(X, step=\"test\"):\n",
    "    if step == \"train\":        \n",
    "        X = np.concatenate((X[:,:51], var_sel.fit_transform(X[:,51:])), axis=1)\n",
    "    else:\n",
    "        X = np.concatenate((X[:,:51], var_sel.transform(X[:,51:])), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Standardization:** Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X, step=\"test\"):\n",
    "    \"\"\"Simple standardization that accepts both a single arr, AND 2 arrays in case of RNN+FeatureEngineering\"\"\"\n",
    "    def standardize(X, step):\n",
    "        if step == \"train\":\n",
    "            return scaler.fit_transform(X)\n",
    "        else:\n",
    "            return scaler.transform(X)\n",
    "\n",
    "    X = np.concatenate((X[:,:51], standardize(X[:,51:], step)), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Select Features:** Use sklearn's feature selection module to discard useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selection(model_path='../experiments/0626002250/model.h5', nb_fe_tokeep=35):\n",
    "    model = joblib.load(model_path)\n",
    "    thresholds = sorted(model.feature_importances_)\n",
    "    thresh = thresholds[-nb_fe_tokeep]\n",
    "    feature_sel = sklearn.feature_selection.SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    return feature_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(fe_arr, display=False):\n",
    "    if display:\n",
    "        pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "        pyplot.show()\n",
    "\n",
    "    select_fe_arr = feature_sel.transform(fe_arr)\n",
    "    return select_fe_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_split(X, y):\n",
    "    columns = [i for i in range(X.shape[1])]\n",
    "    columns[0] = 'neuron_id'\n",
    "    X = pd.DataFrame(data=X, columns=columns)\n",
    "\n",
    "    # Sample xyz random neuron_ids\n",
    "    val_set = np.random.choice(np.unique(X[\"neuron_id\"]), 60, replace=False)\n",
    "    val_idx = X['neuron_id'].isin(val_set)\n",
    "    \n",
    "    # Get X_train y_train, X_val y_val\n",
    "    X_train = X[~val_idx].values\n",
    "    X_val = X[val_idx].values\n",
    "    y_train = y[~val_idx]\n",
    "    y_val = y[val_idx]\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Get Data:** Main function used to create numpy arrays from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(dfx, dfy, \n",
    "            exclude_neuron_id=True, \n",
    "            balancing=\"undersample\",\n",
    "            standardize=False,\n",
    "            differencing=False,\n",
    "            get_tsfresh=False,\n",
    "            get_manual=False,\n",
    "            step=\"train\",\n",
    "            RNN=True,\n",
    "            remove_low_variance=True,\n",
    "            split=0.1,\n",
    "            import_tsfresh=True,\n",
    "            get_sum=True,\n",
    "            extract_nid=True,\n",
    "            feature_selection=True,\n",
    "            **extras):    \n",
    "\n",
    "    y = np.reshape(dfy.values, (dfy.values.shape[0],))\n",
    "    \n",
    "    if not import_tsfresh:\n",
    "        X = dfx.values\n",
    "\n",
    "        # Convert from timeseries to interval\n",
    "        if differencing:\n",
    "            X[:,2:51] -= X[:,1:50]\n",
    "            #X = np.delete(X, 0, 1)\n",
    " \n",
    "    if import_tsfresh:\n",
    "        X = np.genfromtxt(dfx, delimiter=',')\n",
    "        dfx, dfy, dfx_test = import_data()\n",
    "    elif get_tsfresh:\n",
    "        X = extract_tsfresh(X)\n",
    "    elif get_manual:\n",
    "        X = extract_manual(X)\n",
    "\n",
    "    if get_tsfresh and remove_low_variance:\n",
    "        X = remove_low_var(X, step)\n",
    "\n",
    "    if get_sum:\n",
    "        X = sommes(X, step)\n",
    "          \n",
    "    if standardize:\n",
    "        X = standardize_data(X, step)\n",
    "\n",
    "    if split and step != \"test\":\n",
    "        if extract_nid:\n",
    "            X, X_val, y, y_val = custom_split(X, y)\n",
    "        else:\n",
    "            X, X_val, y, y_val = train_test_split(X, y, test_size=split, random_state=42)\n",
    "    else:\n",
    "        X_val = np.ones(X.shape)\n",
    "        y_val = np.ones(y.shape)\n",
    "    \n",
    "    if step != \"test\" and balancing:\n",
    "        X, y = balance_data(X, y, method=balancing, **params)\n",
    "    \n",
    "    # Isolate features and DELETE NEURONID\n",
    "    if get_tsfresh or get_manual:\n",
    "        X = isolate_features(X)\n",
    "        if step != \"test\":\n",
    "            X_val = isolate_features(X_val)\n",
    "\n",
    "    if RNN:\n",
    "        X[0] = X[0][..., np.newaxis]\n",
    "        if step != \"test\":\n",
    "            X_val[0] = X_val[0][..., np.newaxis]\n",
    "    \n",
    "    if extract_nid:\n",
    "        X[1] = group_neurons(X[1])\n",
    "        if step != \"test\":\n",
    "            X_val[1] = group_neurons(X_val[1])\n",
    "        \n",
    "#    if extract_nid:\n",
    "#        check_nid(X, step)\n",
    "        \n",
    "    X[1] = np.delete(X[1], 0, 1)\n",
    "    if step != \"test\":\n",
    "        X_val[1] = np.delete(X_val[1], 0, 1)\n",
    "    \n",
    "    if feature_selection:\n",
    "        X[1] = select_features(X[1])\n",
    "        if step != \"test\":\n",
    "            X_val[1] = select_features(X_val[1])\n",
    "\n",
    "\n",
    "#    np.random.shuffle(X[1])\n",
    " #   np.random.shuffle(X_val[1])\n",
    "    return X, y, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nid(X, step):\n",
    "    # Check (here are some neuron_ids: 53 7229 7770 7002 76678)\n",
    "    dfx, dfy, dfx_test = import_data()\n",
    "    #X_new = dfx[dfx[\"neuron_id\"] == 53].values\n",
    "    X_new = dfx.values\n",
    "    X_new[:, 2:] -= X_new[:,1:50]\n",
    "    X_new = extract_manual(X_new)\n",
    "    X_new = sommes(X_new, step)\n",
    "    X_new = np.delete(X_new, range(0,51), 1)\n",
    "    print(X_new[:2])\n",
    "    #print(step, X_new.shape)\n",
    "    #print(np.mean(X_new, axis=0))\n",
    "    \n",
    "    # Real one\n",
    "    #print(X[1][np.where(X[1][:, 0] == 53)][:2])\n",
    "    print(X[1][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arxiv: [Neural activity classification with machine learning models trained oninterspike interval series data](https://arxiv.org/pdf/1810.03855.pdf) => PCA and KNN\n",
    "* Github: [PySpike: Python library to analyze spike Train](https://github.com/mariomulansky/PySpike) => Obscure mathematical measurements between spike trains\n",
    "* Profil: [Prof expert en spike train analysis](http://xtof.perso.math.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(43)\n",
    "tensorflow.set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'name': \"XGBoost_nid\",\n",
    "    'balancing': None,\n",
    "    'standardize': False,\n",
    "    'differencing': True,\n",
    "    'get_tsfresh': False,\n",
    "    'get_manual': True,\n",
    "    'import_tsfresh': False,\n",
    "    'RNN': False,\n",
    "    'class_weight': True,\n",
    "    'remove_low_variance': False,\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 32,\n",
    "    'split': 0.1,\n",
    "    'get_sum': True,\n",
    "    'extract_nid': False,\n",
    "    'feature_selection': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(params):\n",
    "    global var_sel, scaler, feature_sel\n",
    "    var_sel = feature_selection.VarianceThreshold(threshold=.01)\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    feature_sel = get_selection()\n",
    "    \n",
    "    dfx, dfy, dfx_test = import_data()\n",
    "    \n",
    "    if params['import_tsfresh']:\n",
    "        X = \"../data/features0624214715.csv\"\n",
    "        X_test = \"../data/features0624220241.csv\"\n",
    "    else:\n",
    "        X = dfx\n",
    "        X_test = dfx_test\n",
    "\n",
    "    X_train, y_train, X_val, y_val = getData(X, dfy, step=\"train\", **params)\n",
    "    X_test, _, _, _ = getData(X_test, dfy, step=\"test\", **params)\n",
    "    return X_train, y_train, X_val, y_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model):\n",
    "    # Predict on custom X_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0],))\n",
    "    print (y_pred.shape)\n",
    "    \n",
    "    # Convert sigmoid output to 0s and 1s\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "  \n",
    "    # Format .csv in ENS style\n",
    "    dfy_pred = pd.DataFrame(data=y_pred, columns=[\"TARGET\"], dtype=int)\n",
    "    dfy_pred.index.name = \"ID\"\n",
    "    dfy_pred.index += 16635\n",
    "    return dfy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    #print(metrics.classification_report(y_true, y_pred))\n",
    "    print(\"CKS: \", metrics.cohen_kappa_score(y_true, y_pred))\n",
    "    print(\"ROC_AUC: \", metrics.roc_auc_score(y_true, y_pred))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.21.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test = process_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep-Learning 1: blunt RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create and train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64)           2304        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 50, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 50, 256)      264192      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           2080        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 256)          525312      lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 288)          0           lstm_10[0][0]                    \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            289         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 794,177\n",
      "Trainable params: 794,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if params['get_tsfresh'] or params[\"get_manual\"]:\n",
    "    timestep_nb = X_train[0].shape[1]\n",
    "else:\n",
    "    timestep_nb = X_train.shape[1]\n",
    "spike_per_ts = 1\n",
    "params['cell_nb'] = 256\n",
    "\n",
    "input_tensor = Input(shape=(timestep_nb, spike_per_ts))\n",
    "X = LSTM(params['cell_nb'], return_sequences=True, dropout=params['dropout'])(input_tensor)\n",
    "X = LSTM(params['cell_nb'], return_sequences=False)(X)\n",
    "\n",
    "if params['get_tsfresh'] or params[\"get_manual\"]:\n",
    "    additional_features = X_train[1].shape[1]\n",
    "    fe_input = Input(shape=(additional_features,)) # A tensor containing the engineered features\n",
    "    latent = Dense(64, activation='relu')(fe_input)\n",
    "    latent = Dropout(rate=params['dropout'])(latent)\n",
    "    latent = Dense(32, activation='relu')(latent)\n",
    "    latent = Dropout(rate=params['dropout'])(latent)\n",
    "    input_tensor = [input_tensor, fe_input]\n",
    "    X = concatenate([X, latent])   \n",
    "    \n",
    "output_tensor = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4713 samples, validate on 249 samples\n",
      "Epoch 1/5\n",
      "4713/4713 [==============================] - 73s 15ms/step - loss: 7.2907 - acc: 0.5192 - val_loss: 16.0668 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4713/4713 [==============================] - 66s 14ms/step - loss: 7.0160 - acc: 0.5260 - val_loss: 0.1680 - val_acc: 0.9639\n",
      "Epoch 3/5\n",
      "4713/4713 [==============================] - 66s 14ms/step - loss: 6.6349 - acc: 0.5517 - val_loss: 0.3797 - val_acc: 0.9237\n",
      "Epoch 4/5\n",
      "4713/4713 [==============================] - 69s 15ms/step - loss: 6.4276 - acc: 0.5538 - val_loss: 1.2730 - val_acc: 0.7631\n",
      "Epoch 5/5\n",
      "4713/4713 [==============================] - 65s 14ms/step - loss: 6.6778 - acc: 0.5428 - val_loss: 3.4660 - val_acc: 0.5863\n"
     ]
    }
   ],
   "source": [
    "if params[\"class_weight\"]:\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "else:\n",
    "    class_weights = None\n",
    "if params[\"get_tsfresh\"]:\n",
    "    X_train=list(X_train)\n",
    "model.compile(metrics=['accuracy'], loss='mse', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_split=0.05, class_weight=class_weights, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_params = [\n",
    "    ('batch_size', history.params['batch_size']),\n",
    "    ('epochs', history.params['epochs']),\n",
    "    ('samples', history.params['samples']),\n",
    "    ('val_acc', history.history['val_acc'][-1])\n",
    "    ]\n",
    "\n",
    "params.update(history_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2773,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.34      0.48      2196\n",
      "           1       0.21      0.67      0.32       577\n",
      "\n",
      "    accuracy                           0.41      2773\n",
      "   macro avg       0.50      0.51      0.40      2773\n",
      "weighted avg       0.67      0.41      0.45      2773\n",
      "\n",
      "0.005695317851993842\n"
     ]
    }
   ],
   "source": [
    "if params[\"get_tsfresh\"] or params[\"get_manual\"]:\n",
    "    X_val=list(X_val)\n",
    "dfy_val = predict(X_val, model)\n",
    "evaluate(y_val, dfy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1373, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfy_val[dfy_val == 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"get_tsfresh\"]:\n",
    "    X_test=list(X_test)\n",
    "dfy_pred = predict(X_test, model)\n",
    "dfy_pred[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-knowledge 1: Benchmark = differencing + tsfresh feature engineering + XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2616, 35)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['class_weight']:\n",
    "    scale_pos_weight = np.sum(y_train == 0)/ float(np.sum(y_train == 1))\n",
    "else:\n",
    "    scale_pos_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:   13.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7444083590036045\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "params_XGB = {\n",
    "        'min_child_weight': [1],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.6],\n",
    "        'max_depth': [10],\n",
    "        'n_estimators': [200],\n",
    "        'learning_rate': [0.05],\n",
    "        'gamma': [0.5, 1],\n",
    "        'scale_pos_weight': [scale_pos_weight]\n",
    "        }\n",
    "\n",
    "search = RandomizedSearchCV(model, param_distributions=params_XGB, n_iter=1, scoring='roc_auc', n_jobs=4, \n",
    "                            verbose=1, random_state=1001 )\n",
    "\n",
    "search.fit(X_train[1], y_train)\n",
    "#search.fit(select_X_train, y_train)\n",
    "best_xgb = search.best_estimator_\n",
    "print(search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-660931c16099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     n_jobs=4, verbose=1, random_state=1001, scoring='roc_auc')\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mbest_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kernels = ['linear']\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "degrees = [0, 3, 4, 5, 6]\n",
    "\n",
    "param_grid = {'C': Cs, 'gamma' : gammas, 'kernel': kernels, 'degree': degrees}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    SVC(), param_distributions=param_grid, n_iter = 5, cv = 3,\n",
    "    n_jobs=4, verbose=1, random_state=1001, scoring='roc_auc')\n",
    "\n",
    "search.fit(X_train[1], y_train)\n",
    "best_svm = search.best_estimator_\n",
    "print(search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train RF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True \n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True \n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True \n",
      "[CV] n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True, total=  11.2s\n",
      "[CV] n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True, total=  11.4s\n",
      "[CV] n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True, total=  11.4s\n",
      "[CV] n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False, total=  29.4s\n",
      "[CV] n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False, total=  27.9s\n",
      "[CV] n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False, total=  28.6s\n",
      "[CV] n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True \n",
      "[CV]  n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False, total=  30.1s\n",
      "[CV] n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True \n",
      "[CV]  n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False, total=  29.4s\n",
      "[CV] n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True \n",
      "[CV]  n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False, total=  28.7s\n",
      "[CV] n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False \n",
      "[CV]  n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True, total=  32.3s\n",
      "[CV] n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False \n",
      "[CV]  n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True, total=  32.2s\n",
      "[CV] n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False \n",
      "[CV]  n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True, total=  32.1s\n",
      "[CV] n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False, total=  41.4s\n",
      "[CV] n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False, total=  41.3s\n",
      "[CV] n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False, total=  23.5s\n",
      "[CV]  n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False, total=  40.7s\n",
      "[CV]  n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False, total=  17.5s\n",
      "[CV]  n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False, total=  15.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7551458113728013\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0, class_weight='balanced')\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 800, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 6, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "rf_random.fit(X_train[1], y_train)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "print(rf_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 50,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Select features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = best_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = sorted(best_xgb.feature_importances_)\n",
    "thresh = thresholds[-50]\n",
    "selection = sklearn.feature_selection.SelectFromModel(best_xgb, threshold=thresh, prefit=True)\n",
    "select_X_train = selection.transform(X_train[1])\n",
    "select_X_val = selection.transform(X_val[1])\n",
    "select_X_test = selection.transform(X_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11969,)\n"
     ]
    }
   ],
   "source": [
    "#select_X_test = selection.transform(X_test[1])\n",
    "dfy_pred = predict(X_test[1], best_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1664,)\n",
      "CKS:  0.33446599097779606\n",
      "ROC_AUC:  0.6705451829723674\n",
      "(1664,)\n",
      "CKS:  0.269970761289391\n",
      "ROC_AUC:  0.6117102315160569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#select_X_val = selection.transform(X_val[1])\n",
    "models = [best_xgb, best_random]\n",
    "for model in models:\n",
    "    dfy_val = predict(X_val[1], model)\n",
    "    evaluate(y_val, dfy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-knowledge 2: KNN with SPIKE- and ISI- synchronization distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../experiments/0627210534'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def saveExp(dfy_pred, model, params):\n",
    "    \"\"\" Create directory in which to save predictions, experiment parameters and model object. \"\"\"\n",
    "\n",
    "    directory = \"../experiments/{}\".format(datetime.datetime.now().strftime(\"%m%d%H%M%S\"))\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    dfy_pred.to_csv(directory + '/y_pred.csv', sep=',')\n",
    "    \n",
    "    joblib.dump(model, directory + '/model.h5')\n",
    "    \n",
    "    columns = []\n",
    "    values = []\n",
    "    for k, v in params.items():\n",
    "        columns.append(k)\n",
    "        values.append(v)\n",
    "    params_df = pd.DataFrame(data=[values], columns=columns)\n",
    "    params_df.to_csv(directory + '/params.csv', sep=';')\n",
    "    return directory\n",
    "\n",
    "# Save model\n",
    "saveExp(dfy_pred, best_random, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 50, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 382)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 128)          66560       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           12256       input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 160)          0           lstm_11[0][0]                    \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            161         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 78,977\n",
      "Trainable params: 78,977\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(5963, 50, 1)\n",
      "{'': '0', 'name': 'RF_vanilla', 'exclude_neuron_id': True, 'balancing': 'undersample', 'standardize': True, 'differencing': True, 'get_tsfresh': True, 'get_ISI_SPIKE': False, 'RNN': False, 'C': '1.0', 'cache_size': '200', 'class_weight': '', 'coef0': '0.0', 'decision_function_shape': 'ovr', 'degree': '3', 'gamma': 'auto', 'kernel': 'rbf', 'max_iter': '-1', 'probability': False, 'random_state': '', 'shrinking': True, 'tol': '0.001', 'verbose': False}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-10985a3e444f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mload_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-301-10985a3e444f>\u001b[0m in \u001b[0;36mload_exp\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdfy_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-296-20dd024b6af5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X_test, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Predict on custom X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "def load_exp():\n",
    "    \"\"\" Reproduce saved experience from a directory: load dataset, model, predict on x_test and evaluate. \"\"\"\n",
    "    \n",
    "    for xp in os.scandir(\"../experiments\"):\n",
    "    \n",
    "        if not xp.is_dir():\n",
    "            continue\n",
    "        \n",
    "        model_path, params_path, y_pred_path = sorted(os.scandir(xp.path), key=lambda x: (x.is_dir(), x.name))\n",
    "        model = load_model(model_path.path)\n",
    "        model.summary()\n",
    "        \n",
    "        with open(params_path, mode='r') as infile:\n",
    "            reader = csv.reader(infile, delimiter=';')\n",
    "            keys, values = reader\n",
    "        params = {keys[ix]:values[ix] for ix in range(len(keys))}\n",
    "        for k, v in params.items():\n",
    "            if v == 'True' or v == 'False':\n",
    "                params[k] = v == 'True'\n",
    "        X_train, y_train, X_val, y_val, X_test = process_data(params)\n",
    "        if isinstance(X_train, tuple):\n",
    "            print(X_train[0].shape)\n",
    "        else:\n",
    "            print(X_train.shape)\n",
    "        print(params)\n",
    "        \n",
    "        dfy_val = predict(X_val, model)\n",
    "        evaluate(model)\n",
    "        \n",
    "load_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
