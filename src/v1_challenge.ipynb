{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ContextualVersionConflict",
     "evalue": "(numba 0.36.2 (/anaconda3/lib/python3.6/site-packages), Requirement.parse('numba>=0.37'), {'umap-learn'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-77a65447c34e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/umap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"umap-learn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \"\"\"\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m: (numba 0.36.2 (/anaconda3/lib/python3.6/site-packages), Requirement.parse('numba>=0.37'), {'umap-learn'})"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import datetime\n",
    "import umap\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import keras\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tsfresh import extract_features\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Activation, Dense, concatenate, LSTM, GRU, Dropout\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from random import uniform, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.read_csv('../data/x_train.csv').set_index('ID')\n",
    "dfy = pd.read_csv('../data/y_train.csv').set_index('ID')\n",
    "dfx_test = pd.read_csv('../data/x_test.csv').set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron_id</th>\n",
       "      <th>timestamp_0</th>\n",
       "      <th>timestamp_1</th>\n",
       "      <th>timestamp_2</th>\n",
       "      <th>timestamp_3</th>\n",
       "      <th>timestamp_4</th>\n",
       "      <th>timestamp_5</th>\n",
       "      <th>timestamp_6</th>\n",
       "      <th>timestamp_7</th>\n",
       "      <th>timestamp_8</th>\n",
       "      <th>...</th>\n",
       "      <th>timestamp_40</th>\n",
       "      <th>timestamp_41</th>\n",
       "      <th>timestamp_42</th>\n",
       "      <th>timestamp_43</th>\n",
       "      <th>timestamp_44</th>\n",
       "      <th>timestamp_45</th>\n",
       "      <th>timestamp_46</th>\n",
       "      <th>timestamp_47</th>\n",
       "      <th>timestamp_48</th>\n",
       "      <th>timestamp_49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>5691</td>\n",
       "      <td>0.077914</td>\n",
       "      <td>0.708334</td>\n",
       "      <td>1.009554</td>\n",
       "      <td>1.125147</td>\n",
       "      <td>1.271336</td>\n",
       "      <td>1.299890</td>\n",
       "      <td>1.666290</td>\n",
       "      <td>1.718390</td>\n",
       "      <td>2.381562</td>\n",
       "      <td>...</td>\n",
       "      <td>29.316422</td>\n",
       "      <td>30.307006</td>\n",
       "      <td>31.185741</td>\n",
       "      <td>31.227892</td>\n",
       "      <td>32.320902</td>\n",
       "      <td>32.701000</td>\n",
       "      <td>32.955075</td>\n",
       "      <td>33.016627</td>\n",
       "      <td>34.837705</td>\n",
       "      <td>34.874491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16631</th>\n",
       "      <td>2341</td>\n",
       "      <td>0.485287</td>\n",
       "      <td>0.870193</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>1.733133</td>\n",
       "      <td>1.755243</td>\n",
       "      <td>1.803468</td>\n",
       "      <td>1.841432</td>\n",
       "      <td>1.986925</td>\n",
       "      <td>2.006145</td>\n",
       "      <td>...</td>\n",
       "      <td>17.151013</td>\n",
       "      <td>17.367892</td>\n",
       "      <td>17.727558</td>\n",
       "      <td>18.178916</td>\n",
       "      <td>18.521734</td>\n",
       "      <td>19.492522</td>\n",
       "      <td>19.515122</td>\n",
       "      <td>20.715555</td>\n",
       "      <td>21.217199</td>\n",
       "      <td>21.640693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16632</th>\n",
       "      <td>8046</td>\n",
       "      <td>0.213619</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>1.575419</td>\n",
       "      <td>1.650658</td>\n",
       "      <td>1.700773</td>\n",
       "      <td>1.856047</td>\n",
       "      <td>1.927563</td>\n",
       "      <td>1.950001</td>\n",
       "      <td>2.367852</td>\n",
       "      <td>...</td>\n",
       "      <td>14.064862</td>\n",
       "      <td>14.092407</td>\n",
       "      <td>14.343008</td>\n",
       "      <td>14.428562</td>\n",
       "      <td>14.671081</td>\n",
       "      <td>14.791297</td>\n",
       "      <td>14.847738</td>\n",
       "      <td>14.916361</td>\n",
       "      <td>15.055357</td>\n",
       "      <td>15.192531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16633</th>\n",
       "      <td>6855</td>\n",
       "      <td>2.669642</td>\n",
       "      <td>7.500198</td>\n",
       "      <td>8.710341</td>\n",
       "      <td>8.724346</td>\n",
       "      <td>8.760471</td>\n",
       "      <td>8.770804</td>\n",
       "      <td>8.786001</td>\n",
       "      <td>8.847625</td>\n",
       "      <td>8.885186</td>\n",
       "      <td>...</td>\n",
       "      <td>15.555430</td>\n",
       "      <td>15.698512</td>\n",
       "      <td>15.782122</td>\n",
       "      <td>16.067267</td>\n",
       "      <td>16.499324</td>\n",
       "      <td>16.906955</td>\n",
       "      <td>17.444176</td>\n",
       "      <td>18.704728</td>\n",
       "      <td>22.082864</td>\n",
       "      <td>27.185064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16634</th>\n",
       "      <td>6921</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>0.376280</td>\n",
       "      <td>0.445379</td>\n",
       "      <td>0.762095</td>\n",
       "      <td>0.929728</td>\n",
       "      <td>1.029945</td>\n",
       "      <td>1.622643</td>\n",
       "      <td>1.710542</td>\n",
       "      <td>2.046901</td>\n",
       "      <td>...</td>\n",
       "      <td>23.397511</td>\n",
       "      <td>23.437452</td>\n",
       "      <td>24.217391</td>\n",
       "      <td>24.519848</td>\n",
       "      <td>24.675594</td>\n",
       "      <td>24.764713</td>\n",
       "      <td>24.894686</td>\n",
       "      <td>25.041824</td>\n",
       "      <td>25.071265</td>\n",
       "      <td>28.288743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       neuron_id  timestamp_0  timestamp_1  timestamp_2  timestamp_3  \\\n",
       "ID                                                                     \n",
       "16630       5691     0.077914     0.708334     1.009554     1.125147   \n",
       "16631       2341     0.485287     0.870193     0.959064     1.733133   \n",
       "16632       8046     0.213619     0.290771     1.575419     1.650658   \n",
       "16633       6855     2.669642     7.500198     8.710341     8.724346   \n",
       "16634       6921     0.075924     0.376280     0.445379     0.762095   \n",
       "\n",
       "       timestamp_4  timestamp_5  timestamp_6  timestamp_7  timestamp_8  \\\n",
       "ID                                                                       \n",
       "16630     1.271336     1.299890     1.666290     1.718390     2.381562   \n",
       "16631     1.755243     1.803468     1.841432     1.986925     2.006145   \n",
       "16632     1.700773     1.856047     1.927563     1.950001     2.367852   \n",
       "16633     8.760471     8.770804     8.786001     8.847625     8.885186   \n",
       "16634     0.929728     1.029945     1.622643     1.710542     2.046901   \n",
       "\n",
       "           ...       timestamp_40  timestamp_41  timestamp_42  timestamp_43  \\\n",
       "ID         ...                                                                \n",
       "16630      ...          29.316422     30.307006     31.185741     31.227892   \n",
       "16631      ...          17.151013     17.367892     17.727558     18.178916   \n",
       "16632      ...          14.064862     14.092407     14.343008     14.428562   \n",
       "16633      ...          15.555430     15.698512     15.782122     16.067267   \n",
       "16634      ...          23.397511     23.437452     24.217391     24.519848   \n",
       "\n",
       "       timestamp_44  timestamp_45  timestamp_46  timestamp_47  timestamp_48  \\\n",
       "ID                                                                            \n",
       "16630     32.320902     32.701000     32.955075     33.016627     34.837705   \n",
       "16631     18.521734     19.492522     19.515122     20.715555     21.217199   \n",
       "16632     14.671081     14.791297     14.847738     14.916361     15.055357   \n",
       "16633     16.499324     16.906955     17.444176     18.704728     22.082864   \n",
       "16634     24.675594     24.764713     24.894686     25.041824     25.071265   \n",
       "\n",
       "       timestamp_49  \n",
       "ID                   \n",
       "16630     34.874491  \n",
       "16631     21.640693  \n",
       "16632     15.192531  \n",
       "16633     27.185064  \n",
       "16634     28.288743  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfy.head()\n",
    "dfx.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same number of samples, all good.\n"
     ]
    }
   ],
   "source": [
    "if dfy.shape[0] == dfx.shape[0]:\n",
    "    print(\"Same number of samples, all good.\")\n",
    "else:\n",
    "    print(\"Different number of samples, problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sanity check:** diff etat1/etat2, neuron_id usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Différence entre le nombre d'etats 1 et d'etats 0.\n",
    "dfy.TARGET.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 neuron_ids in common between the train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Should we keep the neuron_id col ?\n",
    "xtest_uniques = dfx_test.neuron_id.unique()\n",
    "x_uniques = dfx.neuron_id.unique()\n",
    "diff = [x for x in x_uniques if x in xtest_uniques]\n",
    "print(\"There are {} neuron_ids in common between the train and test sets.\".format(len(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx.neuron_id.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Balance dataset:** Under-sample class2 to have same number of samples in both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET SEED pour reproducibilité\n",
    "def balance_data(X, y, method=\"undersampling\", split=1):\n",
    "    \"\"\" Return balanced training dataset obtained by undersampling class 2. \"\"\"\n",
    "    # Extract class2 indices from Y\n",
    "    y_class1_ix = np.where(y == 1)[0]\n",
    "    y_class2_ix = np.where(y == 0)[0]\n",
    "\n",
    "    # Under-sample class2 to get balanced classes\n",
    "    if method == \"undersampling\":\n",
    "        y_class2_ix = np.random.choice(y_class2_ix, int(len(y_class1_ix) * split), replace=False)\n",
    "    else:\n",
    "        y_class2_ix = np.random.choice(y_class2_ix, int(len(y_class2_ix) * split), replace=False)\n",
    "\n",
    "    # Split train & val\n",
    "    y_class1_ix = np.random.choice(y_class1_ix, int(len(y_class1_ix) * split), replace=False)\n",
    "\n",
    "    # Concatenate the undersampled_class2_array and the class1_array\n",
    "    balanced_ix = np.concatenate((y_class1_ix, y_class2_ix), axis=0)\n",
    "    np.random.shuffle(balanced_ix)\n",
    "\n",
    "    # Create X_train dataset (Keras will do the val split)\n",
    "    X_train = X[balanced_ix]\n",
    "    y_train = y[balanced_ix]\n",
    "\n",
    "    if method == \"oversampling\":\n",
    "        X_train = np.reshape(X_train, X_train.shape[:2])\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X_train, y_train = ros.fit_sample(X_train, y_train)\n",
    "        X_train = X_train[..., np.newaxis]\n",
    "    \n",
    "    # Create **UNOFFICIAL** X_val containing only 0s.\n",
    "    balanced_ix_val = np.in1d(range(X.shape[0]), balanced_ix)\n",
    "    \n",
    "    X_val = X[~balanced_ix_val]\n",
    "    y_val = y[~balanced_ix_val]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Extract features:** Use tsfresh to perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP !\n",
    "def extract_tsfresh(dfx):\n",
    "    dfx = pd.DataFrame(dfx.stack(), columns=['spike_time']).reset_index()\n",
    "    extracted_features = extract_features(dfx, column_id=\"ID\", column_value=\"spike_time\", column_sort=\"level_1\")\n",
    "    dfx = dfx.pivot(index='ID', columns='level_1', values='spike_time')\n",
    "    dfx.columns = [int(col[col.find('_') + 1:]) for col in dfx.columns]\n",
    "    dfx = dfx.sort_index(axis=1).join(extracted_features)\n",
    "    dfx = dfx.dropna(axis='columns')\n",
    "    fname = \"../data/features{}.csv\".format(datetime.datetime.now().strftime(\"%m%d%H%M%S\"))\n",
    "    dfx.to_csv(fname, sep=',')\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save(fn):\n",
    "    save = pd.read_csv(fn)\n",
    "    save = save.dropna(axis='columns').set_index('ID')\n",
    "    return save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_features(X):\n",
    "    ts_arr = X[:,:50]\n",
    "    fe_arr = X[:,50:]\n",
    "    fe_arr = np.reshape(fe_arr, fe_arr.shape[:-1])\n",
    "    \"\"\"\n",
    "    Obsolete: repeat engineered features for each 50 timestep of a sample\n",
    "    X = np.concatenate((np.repeat(features_arr.transpose(0,2,1), 50, axis=1), nofeat_arr), axis=2)\n",
    "    !!! Will kill 42's mac !!!\n",
    "    \"\"\"\n",
    "    return list((ts_arr, fe_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_var(fe_arr, step=\"test\"):\n",
    "    if step == \"train\":\n",
    "        fe_arr = sel.fit_transform(fe_arr)\n",
    "    else:\n",
    "        fe_arr = sel.transform(fe_arr)\n",
    "    return fe_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unique_col(df):\n",
    "    for col in df.columns:\n",
    "        if len(df[col].unique()) == 1:\n",
    "            df.drop(col,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Standardization:** Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X, step=\"test\"):\n",
    "    \"\"\"Simple standardization that accepts both a single arr, AND 2 arrays in case of RNN+FeatureEngineering\"\"\"\n",
    "    def standardize(X, step):\n",
    "        if step == \"train\":\n",
    "            X = scaler.fit_transform(X)\n",
    "        else:\n",
    "            X = scaler.transform(X)\n",
    "        #X -= np.mean(X, axis=0)\n",
    "        #X /= (np.std(X, axis=0) + sys.float_info.epsilon)\n",
    "        #X = np.nan_to_num(X)\n",
    "        return X\n",
    "\n",
    "    if isinstance(X, list):\n",
    "        X = (X[0], standardize(X[1], step))\n",
    "#    else:\n",
    "#        X = standardize(X, step)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create Xtrain ytrain:** numpy array from df, with dimensions *(sample_nb, timestep_nb, feature_nb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(dfx, dfy, \n",
    "            exclude_neuron_id=True, \n",
    "            balancing=\"undersample\",\n",
    "            standardize=False,\n",
    "            differencing=False,\n",
    "            get_tsfresh=False,\n",
    "            get_ISI_SPIKE=False,\n",
    "            step=\"train\",\n",
    "            RNN=True,\n",
    "            remove_low_variance=True,\n",
    "            split=0.90,\n",
    "            **extras):    \n",
    "   \n",
    "    if isinstance(dfx, str) and get_tsfresh:\n",
    "        dfx = get_save(dfx)\n",
    "        #drop_unique_col(dfx)\n",
    "    else:\n",
    "        if exclude_neuron_id:\n",
    "            # X: Exclude neuron_id column from array\n",
    "            dfx = dfx.drop(columns=['neuron_id'])\n",
    "        if get_tsfresh:\n",
    "            dfx = extract_tsfresh(dfx)\n",
    "    \n",
    "    X = dfx.values\n",
    "    \n",
    "    X = X[..., np.newaxis]\n",
    "    y = np.reshape(dfy.values, (dfy.values.shape[0],))\n",
    "       \n",
    "    if step != \"test\":\n",
    "        X, y, X_val, y_val = balance_data(X, y, method=balancing, split=split)\n",
    "    else:\n",
    "        X_val = np.ones(X.shape)\n",
    "        y_val = np.ones(y.shape)\n",
    "        \n",
    "    # Convert from timeseries to interval\n",
    "    if differencing:\n",
    "        X[:,1:50] -= X[:,:49]\n",
    "        X_val[:,1:50] -= X_val[:,:49]\n",
    " \n",
    "    if get_ISI_SPIKE:\n",
    "        pass\n",
    "    \n",
    "    if get_tsfresh:\n",
    "        X = isolate_features(X)\n",
    "        X_val = isolate_features(X_val)\n",
    "    \n",
    "    if get_tsfresh and remove_low_variance:\n",
    "        X[1] = remove_low_var(X[1], step)\n",
    "        if step != \"test\":\n",
    "            # Otherwise X_val is a made up np.ones array and has 0 variance => BUG\n",
    "            X_val[1] = remove_low_var(X_val[1])\n",
    "        \n",
    "#    elif get_tsfresh:\n",
    "#        X = np.reshape(X, X.shape[:2])\n",
    "        \n",
    "    if standardize:\n",
    "        X = standardize_data(X, step)\n",
    "        if step != \"test\":\n",
    "            # Same as before\n",
    "            X_val = standardize_data(X_val)\n",
    "      \n",
    "    return X, y, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Deprecated:** Concatenate neuron_id to every timestep of a sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntimesteps_arr = dfx.iloc[:,1:].values\\ntimesteps_arr = timesteps_arr[..., np.newaxis]\\ntimesteps_arr.shape\\n\\nneuron_arr = dfx.iloc[:,0].values\\nneuron_arr.shape\\n\\nneuron_arr = np.broadcast_to(neuron_arr[:,None,None], timesteps_arr.shape)\\nfinal_arr = np.concatenate((timesteps_arr,neuron_arr), axis=2)\\nfinal_arr.shape\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : Make a 3d numpy array from our pandas df\n",
    "# Shape = [samples, timestamps, features]\n",
    "\"\"\"\n",
    "timesteps_arr = dfx.iloc[:,1:].values\n",
    "timesteps_arr = timesteps_arr[..., np.newaxis]\n",
    "timesteps_arr.shape\n",
    "\n",
    "neuron_arr = dfx.iloc[:,0].values\n",
    "neuron_arr.shape\n",
    "\n",
    "neuron_arr = np.broadcast_to(neuron_arr[:,None,None], timesteps_arr.shape)\n",
    "final_arr = np.concatenate((timesteps_arr,neuron_arr), axis=2)\n",
    "final_arr.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arxiv: [Neural activity classification with machine learning models trained oninterspike interval series data](https://arxiv.org/pdf/1810.03855.pdf) => PCA and KNN\n",
    "* Github: [PySpike: Python library to analyze spike Train](https://github.com/mariomulansky/PySpike) => Obscure mathematical measurements between spike trains\n",
    "* Profil: [Prof expert en spike train analysis](http://xtof.perso.math.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tensorflow.set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'name': \"XGB_Daddycool\",\n",
    "    'exclude_neuron_id': True,\n",
    "#    'balancing': None,\n",
    "    'balancing': None,\n",
    "    'standardize': True,\n",
    "    'differencing': True,\n",
    "    'get_tsfresh': True,\n",
    "    'get_ISI_SPIKE': False,\n",
    "    'RNN': False,\n",
    "    'class_weight': True,\n",
    "    'remove_low_variance': True,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(params):\n",
    "    global sel, scaler\n",
    "    sel = sklearn.feature_selection.VarianceThreshold(threshold=.02)\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "    if params['get_tsfresh']:\n",
    "        X = \"../data/features0607182541.csv\"\n",
    "        X_test = \"../data/features0607183651.csv\"\n",
    "    else:\n",
    "        X = dfx\n",
    "        X_test = dfx_test\n",
    "\n",
    "    X_train, y_train, X_val, y_val = getData(X, dfy, step=\"train\", **params)\n",
    "    X_test, _, _, _ = getData(X_test, dfy, step=\"test\", **params)\n",
    "    return X_train, y_train, X_val, y_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model):\n",
    "    # Predict on custom X_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0],))\n",
    "    print (y_pred.shape)\n",
    "    \n",
    "    # Convert sigmoid output to 0s and 1s\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "  \n",
    "    # Format .csv in ENS style\n",
    "    dfy_pred = pd.DataFrame(data=y_pred, columns=[\"TARGET\"], dtype=int)\n",
    "    dfy_pred.index.name = \"ID\"\n",
    "    dfy_pred.index += 16635\n",
    "    return dfy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print(metrics.cohen_kappa_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep-Learning 1: blunt RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test = process_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create and train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 256)           264192    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 789,761\n",
      "Trainable params: 789,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if params['get_tsfresh']:\n",
    "    timestep_nb = X_train[0].shape[1]\n",
    "else:\n",
    "    timestep_nb = X_train.shape[1]\n",
    "spike_per_ts = 1\n",
    "params['cell_nb'] = 256\n",
    "\n",
    "input_tensor = Input(shape=(timestep_nb, spike_per_ts))\n",
    "X = LSTM(params['cell_nb'], return_sequences=True, dropout=params['dropout'])(input_tensor)\n",
    "X = LSTM(params['cell_nb'], return_sequences=False)(X)\n",
    "\n",
    "if params['get_tsfresh']:\n",
    "    additional_features = X_train[1].shape[1]\n",
    "    fe_input = Input(shape=(additional_features,)) # A tensor containing the engineered features\n",
    "    latent = Dense(64, activation='relu')(fe_input)\n",
    "    latent = Dropout(rate=params['dropout'])(latent)\n",
    "    latent = Dense(32, activation='relu')(latent)\n",
    "    latent = Dropout(rate=params['dropout'])(latent)\n",
    "    input_tensor = [input_tensor, fe_input]\n",
    "    X = concatenate([X, latent])   \n",
    "    \n",
    "output_tensor = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23216 samples, validate on 1222 samples\n",
      "Epoch 1/5\n",
      "23216/23216 [==============================] - 399s 17ms/step - loss: 0.2467 - acc: 0.5467 - val_loss: 0.2469 - val_acc: 0.6637\n",
      "Epoch 2/5\n",
      "23216/23216 [==============================] - 397s 17ms/step - loss: 0.2408 - acc: 0.5759 - val_loss: 0.2625 - val_acc: 0.7275\n",
      "Epoch 3/5\n",
      "23216/23216 [==============================] - 386s 17ms/step - loss: 0.2359 - acc: 0.5940 - val_loss: 0.1926 - val_acc: 0.8543\n",
      "Epoch 4/5\n",
      "23216/23216 [==============================] - 380s 16ms/step - loss: 0.2245 - acc: 0.6285 - val_loss: 0.1227 - val_acc: 0.8953\n",
      "Epoch 5/5\n",
      "23216/23216 [==============================] - 391s 17ms/step - loss: 0.2107 - acc: 0.6650 - val_loss: 0.1195 - val_acc: 0.8650\n"
     ]
    }
   ],
   "source": [
    "if params[\"class_weight\"]:\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "else:\n",
    "    class_weights = None\n",
    "if params[\"get_tsfresh\"]:\n",
    "    X_train=list(X_train)\n",
    "model.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_split=0.05, class_weight=class_weights, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_params = [\n",
    "    ('batch_size', history.params['batch_size']),\n",
    "    ('epochs', history.params['epochs']),\n",
    "    ('samples', history.params['samples']),\n",
    "    ('val_acc', history.history['val_acc'][-1])\n",
    "    ]\n",
    "\n",
    "params.update(history_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1664,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.52      0.66      1358\n",
      "          1       0.28      0.84      0.42       306\n",
      "\n",
      "avg / total       0.81      0.57      0.62      1664\n",
      "\n",
      "0.19895125626227295\n"
     ]
    }
   ],
   "source": [
    "if params[\"get_tsfresh\"]:\n",
    "    X_val=list(X_val)\n",
    "dfy_val = predict(X_val, model)\n",
    "evaluate(y_val, dfy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11969,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16635</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16636</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16637</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16638</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16639</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16640</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16641</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16642</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16643</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16644</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16645</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16646</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16647</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16648</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TARGET\n",
       "16635       1\n",
       "16636       1\n",
       "16637       0\n",
       "16638       0\n",
       "16639       1\n",
       "16640       0\n",
       "16641       1\n",
       "16642       0\n",
       "16643       0\n",
       "16644       1\n",
       "16645       0\n",
       "16646       0\n",
       "16647       1\n",
       "16648       0\n",
       "16649       0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if params[\"get_tsfresh\"]:\n",
    "    X_test=list(X_test)\n",
    "dfy_pred = predict(X_test, model)\n",
    "dfy_pred[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-knowledge 1: Benchmark = differencing + tsfresh feature engineering + XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nscores = []\\nfor split in np.arange(0.2, 1, 0.1):\\n    params['split'] = split\\n    X_train, y_train, X_val, y_val, X_test = process_data(params)\\n    print(X_train[1].shape)\\n    model.fit(X_train[1], y_train)\\n    dfy_val = predict(X_val[1], model)\\n    scores.append((metrics.f1_score(y_val, dfy_val), metrics.cohen_kappa_score(y_val, dfy_val)))\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "scores = []\n",
    "for split in np.arange(0.2, 1, 0.1):\n",
    "    params['split'] = split\n",
    "    X_train, y_train, X_val, y_val, X_test = process_data(params)\n",
    "    print(X_train[1].shape)\n",
    "    model.fit(X_train[1], y_train)\n",
    "    dfy_val = predict(X_val[1], model)\n",
    "    scores.append((metrics.f1_score(y_val, dfy_val), metrics.cohen_kappa_score(y_val, dfy_val)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test = process_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14971, 299)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['class_weight']:\n",
    "    scale_pos_weight = np.sum(y_train == 0)/ float(np.sum(y_train == 1))\n",
    "else:\n",
    "    scale_pos_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB\n",
    "params_XGB = {\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.5,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'max_depth': 10,\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate': 0.05,\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "        }\n",
    "model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, **params_XGB)\n",
    "\n",
    "#SVC\n",
    "#model = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] subsample=0.8, min_child_weight=1, max_depth=10, gamma=0.5, colsample_bytree=0.6 \n",
      "[CV] subsample=0.8, min_child_weight=1, max_depth=10, gamma=0.5, colsample_bytree=0.6 \n",
      "[CV] subsample=0.8, min_child_weight=1, max_depth=10, gamma=0.5, colsample_bytree=0.6 \n",
      "[CV]  subsample=0.8, min_child_weight=1, max_depth=10, gamma=0.5, colsample_bytree=0.6, score=0.6733275136172264, total=  13.0s\n",
      "[CV]  subsample=0.8, min_child_weight=1, max_depth=10, gamma=0.5, colsample_bytree=0.6, score=0.6633023118705221, total=  13.2s\n",
      "[CV]  subsample=0.8, min_child_weight=1, max_depth=10, gamma=0.5, colsample_bytree=0.6, score=0.6599734774926833, total=  13.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:   14.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.6, gamma=0.5, learning_rate=0.05,\n",
       "       max_delta_step=0, max_depth=10, min_child_weight=1, missing=None,\n",
       "       n_estimators=200, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=42, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=4.440043604651163, seed=None,\n",
       "       silent=True, subsample=0.8),\n",
       "          fit_params=None, iid=True, n_iter=1, n_jobs=4,\n",
       "          param_distributions={'min_child_weight': [1], 'gamma': [0.5, 1], 'subsample': [0.8], 'colsample_bytree': [0.6], 'max_depth': [10]},\n",
       "          pre_dispatch='2*n_jobs', random_state=1001, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.update(model.get_params())\n",
    "params_XGB = {\n",
    "        'min_child_weight': [1],\n",
    "        'gamma': [0.5, 1],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.6],\n",
    "        'max_depth': [10]\n",
    "        }\n",
    "\"\"\"\n",
    "#search = RandomizedSearchCV(model, param_distributions=params_CV, rchandom_state=42,\n",
    "#                            n_iter=200, cv=3, verbose=1, n_jobs=4, return_train_score=True)\n",
    "\"\"\"\n",
    "search = RandomizedSearchCV(model, param_distributions=params_XGB, n_iter=1, scoring='roc_auc', n_jobs=4, \n",
    "                            verbose=3, random_state=1001 )\n",
    "\n",
    "search.fit(select_X_train, y_train)\n",
    "\n",
    "#model.fit(X_train[1], y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb= search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 800, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True \n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True \n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True \n",
      "[CV] n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True, total=  12.4s\n",
      "[CV] n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True, total=  12.5s\n",
      "[CV] n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_features=sqrt, max_depth=50, bootstrap=True, total=  12.9s\n",
      "[CV] n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False, total=  33.7s\n",
      "[CV] n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False, total=  32.7s\n",
      "[CV] n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=333, min_samples_split=10, min_samples_leaf=4, max_features=sqrt, max_depth=90, bootstrap=False, total=  34.1s\n",
      "[CV] n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True \n",
      "[CV]  n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False, total=  33.9s\n",
      "[CV] n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True \n",
      "[CV]  n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False, total=  32.8s\n",
      "[CV] n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True \n",
      "[CV]  n_estimators=333, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=60, bootstrap=False, total=  33.6s\n",
      "[CV] n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False \n",
      "[CV]  n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True, total=  35.5s\n",
      "[CV] n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False \n",
      "[CV]  n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True, total=  35.4s\n",
      "[CV] n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False \n",
      "[CV]  n_estimators=600, min_samples_split=5, min_samples_leaf=1, max_features=sqrt, max_depth=30, bootstrap=True, total=  36.1s\n",
      "[CV] n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False, total=  48.2s\n",
      "[CV] n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False, total=  48.6s\n",
      "[CV] n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False \n",
      "[CV]  n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False, total=  29.1s\n",
      "[CV] n_estimators=800, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=False \n",
      "[CV]  n_estimators=466, min_samples_split=10, min_samples_leaf=1, max_features=auto, max_depth=80, bootstrap=False, total=  50.7s\n",
      "[CV] n_estimators=800, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=False \n",
      "[CV]  n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False, total=  28.4s\n",
      "[CV] n_estimators=800, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=False \n",
      "[CV]  n_estimators=266, min_samples_split=10, min_samples_leaf=1, max_features=sqrt, max_depth=60, bootstrap=False, total=  28.4s\n",
      "[CV] n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_features=sqrt, max_depth=10, bootstrap=True \n",
      "[CV]  n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_features=sqrt, max_depth=10, bootstrap=True, total=   8.7s\n",
      "[CV] n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_features=sqrt, max_depth=10, bootstrap=True \n",
      "[CV]  n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_features=sqrt, max_depth=10, bootstrap=True, total=   8.9s\n",
      "[CV] n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_features=sqrt, max_depth=10, bootstrap=True \n",
      "[CV]  n_estimators=200, min_samples_split=5, min_samples_leaf=2, max_features=sqrt, max_depth=10, bootstrap=True, total=   8.8s\n",
      "[CV] n_estimators=533, min_samples_split=2, min_samples_leaf=4, max_features=auto, max_depth=100, bootstrap=True \n",
      "[CV]  n_estimators=800, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=False, total= 1.3min\n",
      "[CV] n_estimators=533, min_samples_split=2, min_samples_leaf=4, max_features=auto, max_depth=100, bootstrap=True \n",
      "[CV]  n_estimators=800, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=False, total= 1.3min\n",
      "[CV] n_estimators=533, min_samples_split=2, min_samples_leaf=4, max_features=auto, max_depth=100, bootstrap=True \n",
      "[CV]  n_estimators=533, min_samples_split=2, min_samples_leaf=4, max_features=auto, max_depth=100, bootstrap=True, total=  29.8s\n",
      "[CV] n_estimators=800, min_samples_split=5, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=True \n",
      "[CV]  n_estimators=800, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=False, total= 1.4min\n",
      "[CV] n_estimators=800, min_samples_split=5, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=True \n",
      "[CV]  n_estimators=533, min_samples_split=2, min_samples_leaf=4, max_features=auto, max_depth=100, bootstrap=True, total=  31.5s\n",
      "[CV] n_estimators=800, min_samples_split=5, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=True \n",
      "[CV]  n_estimators=533, min_samples_split=2, min_samples_leaf=4, max_features=auto, max_depth=100, bootstrap=True, total=  31.8s\n",
      "[CV]  n_estimators=800, min_samples_split=5, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=True, total=  45.6s\n",
      "[CV]  n_estimators=800, min_samples_split=5, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=True, total=  37.7s\n",
      "[CV]  n_estimators=800, min_samples_split=5, min_samples_leaf=2, max_features=auto, max_depth=50, bootstrap=True, total=  35.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=2, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 266, 333, 400, 466, 533, 600, 666, 733, 800], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "rf_random.fit(select_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11969,)\n"
     ]
    }
   ],
   "source": [
    "select_X_test = selection.transform(X_test[1])\n",
    "dfy_pred = predict(select_X_test, best_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1664,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.94      0.89      1358\n",
      "          1       0.49      0.25      0.33       306\n",
      "\n",
      "avg / total       0.78      0.81      0.79      1664\n",
      "\n",
      "0.2364152135305546\n"
     ]
    }
   ],
   "source": [
    "select_X_val = selection.transform(X_val[1])\n",
    "dfy_val = predict(select_X_val, best_random)\n",
    "evaluate(y_val, dfy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.bar(range(len(best_xgb.feature_importances_)), best_xgb.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = sorted(best_xgb.feature_importances_)\n",
    "thresh = thresholds[-20]\n",
    "selection = sklearn.feature_selection.SelectFromModel(best_xgb, threshold=thresh, prefit=True)\n",
    "select_X_train = selection.transform(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14971, 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-knowledge 2: KNN with SPIKE- and ISI- synchronization distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../experiments/0622164316'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def saveExp(dfy_pred, model, params):\n",
    "    \"\"\" Create directory in which to save predictions, experiment parameters and model object. \"\"\"\n",
    "\n",
    "    directory = \"../experiments/{}\".format(datetime.datetime.now().strftime(\"%m%d%H%M%S\"))\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    dfy_pred.to_csv(directory + '/y_pred.csv', sep=',')\n",
    "    \n",
    "    joblib.dump(model, directory + '/model.h5')\n",
    "    \n",
    "    columns = []\n",
    "    values = []\n",
    "    for k, v in params.items():\n",
    "        columns.append(k)\n",
    "        values.append(v)\n",
    "    params_df = pd.DataFrame(data=[values], columns=columns)\n",
    "    params_df.to_csv(directory + '/params.csv', sep=';')\n",
    "    return directory\n",
    "\n",
    "# Save model\n",
    "saveExp(dfy_pred, best_random, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 50, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 382)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 128)          66560       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           12256       input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 160)          0           lstm_11[0][0]                    \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            161         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 78,977\n",
      "Trainable params: 78,977\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(5963, 50, 1)\n",
      "{'': '0', 'name': 'RF_vanilla', 'exclude_neuron_id': True, 'balancing': 'undersample', 'standardize': True, 'differencing': True, 'get_tsfresh': True, 'get_ISI_SPIKE': False, 'RNN': False, 'C': '1.0', 'cache_size': '200', 'class_weight': '', 'coef0': '0.0', 'decision_function_shape': 'ovr', 'degree': '3', 'gamma': 'auto', 'kernel': 'rbf', 'max_iter': '-1', 'probability': False, 'random_state': '', 'shrinking': True, 'tol': '0.001', 'verbose': False}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-10985a3e444f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mload_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-301-10985a3e444f>\u001b[0m in \u001b[0;36mload_exp\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdfy_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-296-20dd024b6af5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X_test, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Predict on custom X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "def load_exp():\n",
    "    \"\"\" Reproduce saved experience from a directory: load dataset, model, predict on x_test and evaluate. \"\"\"\n",
    "    \n",
    "    for xp in os.scandir(\"../experiments\"):\n",
    "    \n",
    "        if not xp.is_dir():\n",
    "            continue\n",
    "        \n",
    "        model_path, params_path, y_pred_path = sorted(os.scandir(xp.path), key=lambda x: (x.is_dir(), x.name))\n",
    "        model = load_model(model_path.path)\n",
    "        model.summary()\n",
    "        \n",
    "        with open(params_path, mode='r') as infile:\n",
    "            reader = csv.reader(infile, delimiter=';')\n",
    "            keys, values = reader\n",
    "        params = {keys[ix]:values[ix] for ix in range(len(keys))}\n",
    "        for k, v in params.items():\n",
    "            if v == 'True' or v == 'False':\n",
    "                params[k] = v == 'True'\n",
    "        X_train, y_train, X_val, y_val, X_test = process_data(params)\n",
    "        if isinstance(X_train, tuple):\n",
    "            print(X_train[0].shape)\n",
    "        else:\n",
    "            print(X_train.shape)\n",
    "        print(params)\n",
    "        \n",
    "        dfy_val = predict(X_val, model)\n",
    "        evaluate(model)\n",
    "        \n",
    "load_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
