{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input, Activation, Dense, Dropout, Flatten, Lambda, LSTM, GRU\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.read_csv('../data/x_train.csv').set_index('ID')\n",
    "dfy = pd.read_csv('../data/y_train.csv').set_index('ID')\n",
    "dfx_test = pd.read_csv('../data/x_test.csv').set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron_id</th>\n",
       "      <th>timestamp_0</th>\n",
       "      <th>timestamp_1</th>\n",
       "      <th>timestamp_2</th>\n",
       "      <th>timestamp_3</th>\n",
       "      <th>timestamp_4</th>\n",
       "      <th>timestamp_5</th>\n",
       "      <th>timestamp_6</th>\n",
       "      <th>timestamp_7</th>\n",
       "      <th>timestamp_8</th>\n",
       "      <th>...</th>\n",
       "      <th>timestamp_40</th>\n",
       "      <th>timestamp_41</th>\n",
       "      <th>timestamp_42</th>\n",
       "      <th>timestamp_43</th>\n",
       "      <th>timestamp_44</th>\n",
       "      <th>timestamp_45</th>\n",
       "      <th>timestamp_46</th>\n",
       "      <th>timestamp_47</th>\n",
       "      <th>timestamp_48</th>\n",
       "      <th>timestamp_49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28599</th>\n",
       "      <td>9456</td>\n",
       "      <td>1.501446</td>\n",
       "      <td>1.935893</td>\n",
       "      <td>2.100466</td>\n",
       "      <td>2.144410</td>\n",
       "      <td>2.259503</td>\n",
       "      <td>2.269867</td>\n",
       "      <td>2.292989</td>\n",
       "      <td>2.346420</td>\n",
       "      <td>2.724702</td>\n",
       "      <td>...</td>\n",
       "      <td>20.970412</td>\n",
       "      <td>20.984293</td>\n",
       "      <td>20.997030</td>\n",
       "      <td>21.016212</td>\n",
       "      <td>21.039446</td>\n",
       "      <td>21.044450</td>\n",
       "      <td>21.277555</td>\n",
       "      <td>21.723172</td>\n",
       "      <td>22.037007</td>\n",
       "      <td>22.061580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28600</th>\n",
       "      <td>10064</td>\n",
       "      <td>0.059309</td>\n",
       "      <td>0.116247</td>\n",
       "      <td>0.163038</td>\n",
       "      <td>0.262012</td>\n",
       "      <td>0.301379</td>\n",
       "      <td>1.813054</td>\n",
       "      <td>2.068066</td>\n",
       "      <td>2.168285</td>\n",
       "      <td>2.232430</td>\n",
       "      <td>...</td>\n",
       "      <td>21.518761</td>\n",
       "      <td>22.137357</td>\n",
       "      <td>22.504569</td>\n",
       "      <td>23.102718</td>\n",
       "      <td>23.155953</td>\n",
       "      <td>23.254136</td>\n",
       "      <td>23.302653</td>\n",
       "      <td>23.480564</td>\n",
       "      <td>23.539625</td>\n",
       "      <td>23.585673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28601</th>\n",
       "      <td>10045</td>\n",
       "      <td>0.563164</td>\n",
       "      <td>1.563625</td>\n",
       "      <td>1.677879</td>\n",
       "      <td>1.881264</td>\n",
       "      <td>2.583920</td>\n",
       "      <td>2.948714</td>\n",
       "      <td>3.112856</td>\n",
       "      <td>3.299712</td>\n",
       "      <td>6.040656</td>\n",
       "      <td>...</td>\n",
       "      <td>22.736962</td>\n",
       "      <td>22.842219</td>\n",
       "      <td>22.905865</td>\n",
       "      <td>22.971377</td>\n",
       "      <td>22.982869</td>\n",
       "      <td>23.614314</td>\n",
       "      <td>23.650248</td>\n",
       "      <td>23.807894</td>\n",
       "      <td>24.425525</td>\n",
       "      <td>25.271358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28602</th>\n",
       "      <td>9570</td>\n",
       "      <td>1.052182</td>\n",
       "      <td>1.066127</td>\n",
       "      <td>1.805688</td>\n",
       "      <td>1.824387</td>\n",
       "      <td>1.864275</td>\n",
       "      <td>2.420648</td>\n",
       "      <td>2.431845</td>\n",
       "      <td>2.757334</td>\n",
       "      <td>3.235232</td>\n",
       "      <td>...</td>\n",
       "      <td>35.510581</td>\n",
       "      <td>35.700678</td>\n",
       "      <td>37.062503</td>\n",
       "      <td>37.789828</td>\n",
       "      <td>38.569837</td>\n",
       "      <td>39.213227</td>\n",
       "      <td>41.504263</td>\n",
       "      <td>41.963118</td>\n",
       "      <td>41.983141</td>\n",
       "      <td>42.038623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28603</th>\n",
       "      <td>9836</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.026412</td>\n",
       "      <td>0.033590</td>\n",
       "      <td>0.307083</td>\n",
       "      <td>0.311297</td>\n",
       "      <td>0.426553</td>\n",
       "      <td>0.449229</td>\n",
       "      <td>0.453311</td>\n",
       "      <td>1.199124</td>\n",
       "      <td>...</td>\n",
       "      <td>17.048710</td>\n",
       "      <td>17.993623</td>\n",
       "      <td>18.675683</td>\n",
       "      <td>18.827421</td>\n",
       "      <td>18.847187</td>\n",
       "      <td>18.923304</td>\n",
       "      <td>18.929989</td>\n",
       "      <td>19.166302</td>\n",
       "      <td>19.362858</td>\n",
       "      <td>19.370905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       neuron_id  timestamp_0  timestamp_1  timestamp_2  timestamp_3  \\\n",
       "ID                                                                     \n",
       "28599       9456     1.501446     1.935893     2.100466     2.144410   \n",
       "28600      10064     0.059309     0.116247     0.163038     0.262012   \n",
       "28601      10045     0.563164     1.563625     1.677879     1.881264   \n",
       "28602       9570     1.052182     1.066127     1.805688     1.824387   \n",
       "28603       9836     0.004843     0.026412     0.033590     0.307083   \n",
       "\n",
       "       timestamp_4  timestamp_5  timestamp_6  timestamp_7  timestamp_8  \\\n",
       "ID                                                                       \n",
       "28599     2.259503     2.269867     2.292989     2.346420     2.724702   \n",
       "28600     0.301379     1.813054     2.068066     2.168285     2.232430   \n",
       "28601     2.583920     2.948714     3.112856     3.299712     6.040656   \n",
       "28602     1.864275     2.420648     2.431845     2.757334     3.235232   \n",
       "28603     0.311297     0.426553     0.449229     0.453311     1.199124   \n",
       "\n",
       "           ...       timestamp_40  timestamp_41  timestamp_42  timestamp_43  \\\n",
       "ID         ...                                                                \n",
       "28599      ...          20.970412     20.984293     20.997030     21.016212   \n",
       "28600      ...          21.518761     22.137357     22.504569     23.102718   \n",
       "28601      ...          22.736962     22.842219     22.905865     22.971377   \n",
       "28602      ...          35.510581     35.700678     37.062503     37.789828   \n",
       "28603      ...          17.048710     17.993623     18.675683     18.827421   \n",
       "\n",
       "       timestamp_44  timestamp_45  timestamp_46  timestamp_47  timestamp_48  \\\n",
       "ID                                                                            \n",
       "28599     21.039446     21.044450     21.277555     21.723172     22.037007   \n",
       "28600     23.155953     23.254136     23.302653     23.480564     23.539625   \n",
       "28601     22.982869     23.614314     23.650248     23.807894     24.425525   \n",
       "28602     38.569837     39.213227     41.504263     41.963118     41.983141   \n",
       "28603     18.847187     18.923304     18.929989     19.166302     19.362858   \n",
       "\n",
       "       timestamp_49  \n",
       "ID                   \n",
       "28599     22.061580  \n",
       "28600     23.585673  \n",
       "28601     25.271358  \n",
       "28602     42.038623  \n",
       "28603     19.370905  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfy.head()\n",
    "dfx_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same number of samples, all good.\n"
     ]
    }
   ],
   "source": [
    "if dfy.shape[0] == dfx.shape[0]:\n",
    "    print(\"Same number of samples, all good.\")\n",
    "else:\n",
    "    print(\"Different number of samples, problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sanity check:** diff etat1/etat2, neuron_id usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DiffÃ©rence entre le nombre d'etats 1 et d'etats 0.\n",
    "dfy.TARGET.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 neuron_ids in common between the train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Should we keep the neuron_id col ?\n",
    "xtest_uniques = dfx_test.neuron_id.unique()\n",
    "x_uniques = dfx.neuron_id.unique()\n",
    "diff = [x for x in x_uniques if x in xtest_uniques]\n",
    "print(\"There are {} neuron_ids in common between the train and test sets.\".format(len(diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Balance dataset:** Under-sample class2 to have same number of samples in both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET SEED pour reproducibilitÃ©\n",
    "\n",
    "def undersample(X, y):\n",
    "    \"\"\" Return balanced training dataset obtained by undersampling class 2. \"\"\"\n",
    "    # Extract class2 indices from Y\n",
    "    y_class1_ix = np.where(y == 1)[0]\n",
    "    y_class2_ix = np.where(y == 0)[0]\n",
    "    \n",
    "    # Under-sample class2 to get balanced classes\n",
    "    y_class2_ix_undersampled = np.random.choice(y_class2_ix, len(y_class1_ix))\n",
    "\n",
    "    # Concatenate the undersampled_class2_array and the class1_array\n",
    "    balanced_ix = np.concatenate((y_class1_ix, y_class2_ix_undersampled), axis=0)\n",
    "    np.random.shuffle(balanced_ix)\n",
    "    \n",
    "    # Create X_train dataset (Keras will do the val split)\n",
    "    X_train = X[balanced_ix]\n",
    "    y_train = y[balanced_ix]\n",
    "    \n",
    "    \"\"\" DEPRECATED:\n",
    "    # Create **UNOFFICIAL** X_test containing only 0s.\n",
    "    balanced_ix_test = np.in1d(range(X.shape[0]), balanced_ix)\n",
    "    X_test = X[~balanced_ix_test]\n",
    "    y_test = y[~balanced_ix_test]\"\"\"\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Extract features:** Use tsfresh to perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP !\n",
    "def extract_tsfresh(dfx):\n",
    "    dfx = pd.DataFrame(dfx.stack(), columns=['spike_time']).reset_index()\n",
    "    extracted_features = extract_features(dfx, column_id=\"ID\", column_value=\"spike_time\", column_sort=\"level_1\")\n",
    "    dfx = dfx.pivot(index='ID', columns='level_1', values='spike_time')\n",
    "    dfx.columns = [int(col[col.find('_') + 1:]) for col in dfx.columns]\n",
    "    dfx = dfx.sort_index(axis=1).join(extracted_features)\n",
    "    dfx = dfx.dropna(axis='columns')\n",
    "    fname = \"../data/features{}.csv\".format(datetime.datetime.now().strftime(\"%m%d%H%M%S\"))\n",
    "    dfx.to_csv(fname, sep=',')\n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save(fn):\n",
    "    save = pd.read_csv(fn)\n",
    "    save = save.dropna(axis='columns').set_index('ID')\n",
    "    return save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_features(X):\n",
    "    features_arr = X[:,50:]\n",
    "    nofeat_arr = np.delete(X, range(50,532), 1)\n",
    "    X = np.concatenate((np.repeat(features_arr.transpose(0,2,1), 50, axis=1), nofeat_arr), axis=2)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unique_col(df):\n",
    "    for col in df.columns:\n",
    "        if len(df[col].unique()) == 1:\n",
    "            df.drop(col,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Standardization:** Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "    X -= np.mean(X, axis=0)\n",
    "    X /= (np.std(X, axis=0) + sys.float_info.epsilon)\n",
    "    X = np.nan_to_num(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create Xtrain ytrain:** numpy array from df, with dimensions *(sample_nb, timestep_nb, feature_nb)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(dfx, dfy, exclude_neuron_id=True, balancing=\"undersample\", standardize=False, differencing=False, get_tsfresh=False, get_ISI_SPIKE=False, testing=False, **extras):    \n",
    "   \n",
    "    if isinstance(dfx, str):\n",
    "        dfx = get_save(dfx)\n",
    "        drop_unique_col(dfx)\n",
    "    else:\n",
    "        if exclude_neuron_id:\n",
    "            # X: Exclude neuron_id column from array\n",
    "            dfx = dfx.drop(columns=['neuron_id'])\n",
    "        if get_tsfresh:\n",
    "            dfx = extract_tsfresh(dfx)\n",
    "    \n",
    "    X = dfx.values\n",
    "    \n",
    "    X = X[..., np.newaxis]\n",
    "    y = np.reshape(dfy.values, (dfy.values.shape[0],))\n",
    "       \n",
    "    if balancing == \"undersample\" and not testing:\n",
    "        X, y = undersample(X, y)\n",
    "        \n",
    "    # Convert from timeseries to interval\n",
    "    if differencing:\n",
    "        X[:,1:50] -= X[:,:49]\n",
    " \n",
    "    if get_ISI_SPIKE:\n",
    "        pass\n",
    "    \n",
    "    if get_ISI_SPIKE or get_tsfresh:\n",
    "        X = repeat_features(X)\n",
    "\n",
    "    print (\"Before: \\n\", np.argwhere(np.isnan(X)))\n",
    "        \n",
    "    if standardize:\n",
    "        X = standardize_data(X)\n",
    "      \n",
    "    print (\"After: \\n\", np.argwhere(np.isnan(X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Deprecated:** Concatenate neuron_id to every timestep of a sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntimesteps_arr = dfx.iloc[:,1:].values\\ntimesteps_arr = timesteps_arr[..., np.newaxis]\\ntimesteps_arr.shape\\n\\nneuron_arr = dfx.iloc[:,0].values\\nneuron_arr.shape\\n\\nneuron_arr = np.broadcast_to(neuron_arr[:,None,None], timesteps_arr.shape)\\nfinal_arr = np.concatenate((timesteps_arr,neuron_arr), axis=2)\\nfinal_arr.shape\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : Make a 3d numpy array from our pandas df\n",
    "# Shape = [samples, timestamps, features]\n",
    "\"\"\"\n",
    "timesteps_arr = dfx.iloc[:,1:].values\n",
    "timesteps_arr = timesteps_arr[..., np.newaxis]\n",
    "timesteps_arr.shape\n",
    "\n",
    "neuron_arr = dfx.iloc[:,0].values\n",
    "neuron_arr.shape\n",
    "\n",
    "neuron_arr = np.broadcast_to(neuron_arr[:,None,None], timesteps_arr.shape)\n",
    "final_arr = np.concatenate((timesteps_arr,neuron_arr), axis=2)\n",
    "final_arr.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arxiv: [Neural activity classification with machine learning models trained oninterspike interval series data](https://arxiv.org/pdf/1810.03855.pdf) => PCA and KNN\n",
    "* Github: [PySpike: Python library to analyze spike Train](https://github.com/mariomulansky/PySpike) => Obscure mathematical measurements between spike trains\n",
    "* Profil: [Prof expert en spike train analysis](http://xtof.perso.math.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'name': \"LSTM_TSFresh_dropunique\",\n",
    "    'exclude_neuron_id': True,\n",
    "    'balancing': \"undersample\",\n",
    "    'standardize': True,\n",
    "    'differencing': True,\n",
    "    'get_tsfresh': True,\n",
    "    'get_ISI_SPIKE': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      " []\n",
      "After: \n",
      " []\n",
      "Before: \n",
      " []\n",
      "After: \n",
      " []\n",
      "(6116, 432, 1) (11969, 432, 1)\n"
     ]
    }
   ],
   "source": [
    "if params['get_tsfresh']:\n",
    "    X_train, y_train = getData(dfx, dfy, **params)\n",
    "    X_test, _ = getData(dfx_test, dfy, testing=True, **params)\n",
    "else:\n",
    "    X_train, y_train = getData(\"../data/features0607182541.csv\", dfy, **params)\n",
    "    X_test, _ = getData(\"../data/features0607183651.csv\", dfy, testing=True, **params)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep-Learning 1: blunt RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create and train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50, 1)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 66,689\n",
      "Trainable params: 66,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "timestamp_nb = X_train.shape[1]\n",
    "feature_nb = X_train.shape[2]\n",
    "params['cell_nb'] = 128\n",
    "\n",
    "input_shape = (timestamp_nb, feature_nb)\n",
    "x = input_tensor = Input(input_shape)\n",
    "x = LSTM(params['cell_nb'], return_sequences=False)(x)\n",
    "x = output_tensor = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /goinfre/miniconda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5504 samples, validate on 612 samples\n",
      "Epoch 1/3\n",
      "5504/5504 [==============================] - 24s 4ms/step - loss: 0.2372 - acc: 0.5987 - val_loss: 0.2312 - val_acc: 0.6324\n",
      "Epoch 2/3\n",
      "5504/5504 [==============================] - 22s 4ms/step - loss: 0.2331 - acc: 0.6250 - val_loss: 0.2301 - val_acc: 0.6258\n",
      "Epoch 3/3\n",
      "5504/5504 [==============================] - 22s 4ms/step - loss: 0.2279 - acc: 0.6343 - val_loss: 0.2304 - val_acc: 0.6405\n"
     ]
    }
   ],
   "source": [
    "model.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_params = [\n",
    "    ('batch_size', history.params['batch_size']),\n",
    "    ('epochs', history.params['epochs']),\n",
    "    ('samples', history.params['samples']),\n",
    "    ('val_acc', history.history['val_acc'][-1])\n",
    "    ]\n",
    "\n",
    "for param in history_params:\n",
    "    params[param[0]] = param[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11969, 50, 1)\n",
      "(11969,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16635</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16636</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16637</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16638</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16639</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16640</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16641</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16642</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16643</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16644</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16645</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16646</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16647</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16648</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16652</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16653</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16654</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16655</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16656</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16657</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16658</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16659</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TARGET\n",
       "ID           \n",
       "16635       1\n",
       "16636       1\n",
       "16637       1\n",
       "16638       1\n",
       "16639       1\n",
       "16640       0\n",
       "16641       1\n",
       "16642       1\n",
       "16643       1\n",
       "16644       1\n",
       "16645       0\n",
       "16646       0\n",
       "16647       1\n",
       "16648       0\n",
       "16649       0\n",
       "16650       0\n",
       "16651       1\n",
       "16652       0\n",
       "16653       1\n",
       "16654       0\n",
       "16655       0\n",
       "16656       0\n",
       "16657       0\n",
       "16658       1\n",
       "16659       0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(X_test):\n",
    "    # Predict on custom X_test\n",
    "    print (X_test.shape)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0],))\n",
    "    print (y_pred.shape)\n",
    "    # Convert sigmoid output to 0s and 1s\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "  \n",
    "    # Format .csv in ENS style\n",
    "    dfy_pred = pd.DataFrame(data=y_pred, columns=[\"TARGET\"], dtype=int)\n",
    "    dfy_pred.index.name = \"ID\"\n",
    "    dfy_pred.index += 16635\n",
    "    return dfy_pred\n",
    "\n",
    "dfy_pred = predict(X_test)\n",
    "dfy_pred[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-knowledge 1: Benchmark = differencing + tsfresh feature engineering + random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-fecce498ece4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    144\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    145\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/goinfre/miniconda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 539\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-knowledge 2: KNN with SPIKE- and ISI- synchronization distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../experiments/0610191544'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def saveExp(dfy_pred, model, params):\n",
    "    \"\"\" Create directory in which to save predictions, experiment parameters and model object. \"\"\"\n",
    "\n",
    "    directory = \"../experiments/{}\".format(datetime.datetime.now().strftime(\"%m%d%H%M%S\"))\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    dfy_pred.to_csv(directory + '/y_pred.csv', sep=',')\n",
    "    \n",
    "    model.save(directory + '/model.h5')\n",
    "    \n",
    "    columns = []\n",
    "    values = []\n",
    "    for k, v in params.items():\n",
    "        columns.append(k)\n",
    "        values.append(v)\n",
    "    params_df = pd.DataFrame(data=[values], columns=columns)\n",
    "    params_df.to_csv(directory + '/params.csv', sep=';')\n",
    "    return directory\n",
    "\n",
    "# Save model\n",
    "saveExp(dfy_pred, model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
