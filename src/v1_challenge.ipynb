{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/goinfre/miniconda/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import datetime\n",
    "import umap\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import keras\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tsfresh import extract_features\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Activation, Dense, concatenate, LSTM, GRU, Dropout\n",
    "from sklearn import metrics, feature_selection\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from random import uniform, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    dfx = pd.read_csv('../data/x_train.csv').set_index('ID').drop(columns=['neuron_id'])\n",
    "    dfy = pd.read_csv('../data/y_train.csv').set_index('ID')\n",
    "    dfx_test = pd.read_csv('../data/x_test.csv').set_index('ID').drop(columns=['neuron_id'])\n",
    "    return dfx, dfy, dfx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron_id</th>\n",
       "      <th>timestamp_0</th>\n",
       "      <th>timestamp_1</th>\n",
       "      <th>timestamp_2</th>\n",
       "      <th>timestamp_3</th>\n",
       "      <th>timestamp_4</th>\n",
       "      <th>timestamp_5</th>\n",
       "      <th>timestamp_6</th>\n",
       "      <th>timestamp_7</th>\n",
       "      <th>timestamp_8</th>\n",
       "      <th>...</th>\n",
       "      <th>timestamp_40</th>\n",
       "      <th>timestamp_41</th>\n",
       "      <th>timestamp_42</th>\n",
       "      <th>timestamp_43</th>\n",
       "      <th>timestamp_44</th>\n",
       "      <th>timestamp_45</th>\n",
       "      <th>timestamp_46</th>\n",
       "      <th>timestamp_47</th>\n",
       "      <th>timestamp_48</th>\n",
       "      <th>timestamp_49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>5691</td>\n",
       "      <td>0.077914</td>\n",
       "      <td>0.708334</td>\n",
       "      <td>1.009554</td>\n",
       "      <td>1.125147</td>\n",
       "      <td>1.271336</td>\n",
       "      <td>1.299890</td>\n",
       "      <td>1.666290</td>\n",
       "      <td>1.718390</td>\n",
       "      <td>2.381562</td>\n",
       "      <td>...</td>\n",
       "      <td>29.316422</td>\n",
       "      <td>30.307006</td>\n",
       "      <td>31.185741</td>\n",
       "      <td>31.227892</td>\n",
       "      <td>32.320902</td>\n",
       "      <td>32.701000</td>\n",
       "      <td>32.955075</td>\n",
       "      <td>33.016627</td>\n",
       "      <td>34.837705</td>\n",
       "      <td>34.874491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16631</th>\n",
       "      <td>2341</td>\n",
       "      <td>0.485287</td>\n",
       "      <td>0.870193</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>1.733133</td>\n",
       "      <td>1.755243</td>\n",
       "      <td>1.803468</td>\n",
       "      <td>1.841432</td>\n",
       "      <td>1.986925</td>\n",
       "      <td>2.006145</td>\n",
       "      <td>...</td>\n",
       "      <td>17.151013</td>\n",
       "      <td>17.367892</td>\n",
       "      <td>17.727558</td>\n",
       "      <td>18.178916</td>\n",
       "      <td>18.521734</td>\n",
       "      <td>19.492522</td>\n",
       "      <td>19.515122</td>\n",
       "      <td>20.715555</td>\n",
       "      <td>21.217199</td>\n",
       "      <td>21.640693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16632</th>\n",
       "      <td>8046</td>\n",
       "      <td>0.213619</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>1.575419</td>\n",
       "      <td>1.650658</td>\n",
       "      <td>1.700773</td>\n",
       "      <td>1.856047</td>\n",
       "      <td>1.927563</td>\n",
       "      <td>1.950001</td>\n",
       "      <td>2.367852</td>\n",
       "      <td>...</td>\n",
       "      <td>14.064862</td>\n",
       "      <td>14.092407</td>\n",
       "      <td>14.343008</td>\n",
       "      <td>14.428562</td>\n",
       "      <td>14.671081</td>\n",
       "      <td>14.791297</td>\n",
       "      <td>14.847738</td>\n",
       "      <td>14.916361</td>\n",
       "      <td>15.055357</td>\n",
       "      <td>15.192531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16633</th>\n",
       "      <td>6855</td>\n",
       "      <td>2.669642</td>\n",
       "      <td>7.500198</td>\n",
       "      <td>8.710341</td>\n",
       "      <td>8.724346</td>\n",
       "      <td>8.760471</td>\n",
       "      <td>8.770804</td>\n",
       "      <td>8.786001</td>\n",
       "      <td>8.847625</td>\n",
       "      <td>8.885186</td>\n",
       "      <td>...</td>\n",
       "      <td>15.555430</td>\n",
       "      <td>15.698512</td>\n",
       "      <td>15.782122</td>\n",
       "      <td>16.067267</td>\n",
       "      <td>16.499324</td>\n",
       "      <td>16.906955</td>\n",
       "      <td>17.444176</td>\n",
       "      <td>18.704728</td>\n",
       "      <td>22.082864</td>\n",
       "      <td>27.185064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16634</th>\n",
       "      <td>6921</td>\n",
       "      <td>0.075924</td>\n",
       "      <td>0.376280</td>\n",
       "      <td>0.445379</td>\n",
       "      <td>0.762095</td>\n",
       "      <td>0.929728</td>\n",
       "      <td>1.029945</td>\n",
       "      <td>1.622643</td>\n",
       "      <td>1.710542</td>\n",
       "      <td>2.046901</td>\n",
       "      <td>...</td>\n",
       "      <td>23.397511</td>\n",
       "      <td>23.437452</td>\n",
       "      <td>24.217391</td>\n",
       "      <td>24.519848</td>\n",
       "      <td>24.675594</td>\n",
       "      <td>24.764713</td>\n",
       "      <td>24.894686</td>\n",
       "      <td>25.041824</td>\n",
       "      <td>25.071265</td>\n",
       "      <td>28.288743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       neuron_id  timestamp_0  timestamp_1  timestamp_2  timestamp_3  \\\n",
       "ID                                                                     \n",
       "16630       5691     0.077914     0.708334     1.009554     1.125147   \n",
       "16631       2341     0.485287     0.870193     0.959064     1.733133   \n",
       "16632       8046     0.213619     0.290771     1.575419     1.650658   \n",
       "16633       6855     2.669642     7.500198     8.710341     8.724346   \n",
       "16634       6921     0.075924     0.376280     0.445379     0.762095   \n",
       "\n",
       "       timestamp_4  timestamp_5  timestamp_6  timestamp_7  timestamp_8  \\\n",
       "ID                                                                       \n",
       "16630     1.271336     1.299890     1.666290     1.718390     2.381562   \n",
       "16631     1.755243     1.803468     1.841432     1.986925     2.006145   \n",
       "16632     1.700773     1.856047     1.927563     1.950001     2.367852   \n",
       "16633     8.760471     8.770804     8.786001     8.847625     8.885186   \n",
       "16634     0.929728     1.029945     1.622643     1.710542     2.046901   \n",
       "\n",
       "           ...       timestamp_40  timestamp_41  timestamp_42  timestamp_43  \\\n",
       "ID         ...                                                                \n",
       "16630      ...          29.316422     30.307006     31.185741     31.227892   \n",
       "16631      ...          17.151013     17.367892     17.727558     18.178916   \n",
       "16632      ...          14.064862     14.092407     14.343008     14.428562   \n",
       "16633      ...          15.555430     15.698512     15.782122     16.067267   \n",
       "16634      ...          23.397511     23.437452     24.217391     24.519848   \n",
       "\n",
       "       timestamp_44  timestamp_45  timestamp_46  timestamp_47  timestamp_48  \\\n",
       "ID                                                                            \n",
       "16630     32.320902     32.701000     32.955075     33.016627     34.837705   \n",
       "16631     18.521734     19.492522     19.515122     20.715555     21.217199   \n",
       "16632     14.671081     14.791297     14.847738     14.916361     15.055357   \n",
       "16633     16.499324     16.906955     17.444176     18.704728     22.082864   \n",
       "16634     24.675594     24.764713     24.894686     25.041824     25.071265   \n",
       "\n",
       "       timestamp_49  \n",
       "ID                   \n",
       "16630     34.874491  \n",
       "16631     21.640693  \n",
       "16632     15.192531  \n",
       "16633     27.185064  \n",
       "16634     28.288743  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx, dfy, dfx_test = import_data()\n",
    "dfy.head()\n",
    "dfx.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same number of samples, all good.\n"
     ]
    }
   ],
   "source": [
    "if dfy.shape[0] == dfx.shape[0]:\n",
    "    print(\"Same number of samples, all good.\")\n",
    "else:\n",
    "    print(\"Different number of samples, problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sanity check:** diff etat1/etat2, neuron_id usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DiffÃ©rence entre le nombre d'etats 1 et d'etats 0.\n",
    "dfy.TARGET.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 neuron_ids in common between the train and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Should we keep the neuron_id col ?\n",
    "xtest_uniques = dfx_test.neuron_id.unique()\n",
    "x_uniques = dfx.neuron_id.unique()\n",
    "diff = [x for x in x_uniques if x in xtest_uniques]\n",
    "print(\"There are {} neuron_ids in common between the train and test sets.\".format(len(diff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Balance dataset:** Choose between undersampling and oversampling, to get equal nb of samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef balance_data(X, y, method=\"undersampling\", split=1, **params):\\n    \"\"\" Return balanced training dataset obtained by undersampling class 2. \"\"\"\\n    y_class1_ix = np.where(y == 1)[0]\\n    y_class2_ix = np.where(y == 0)[0]\\n\\n    # Under-sample class2 to get balanced classes\\n    if method == \"undersampling\":\\n        y_class2_ix = np.random.choice(y_class2_ix, int(len(y_class1_ix) * split), replace=False)\\n    else:\\n        y_class2_ix = np.random.choice(y_class2_ix, int(len(y_class2_ix) * split), replace=False)\\n\\n    # Split train & val\\n    y_class1_ix = np.random.choice(y_class1_ix, int(len(y_class1_ix) * split), replace=False)\\n\\n    # Concatenate the undersampled_class2_array and the class1_array\\n    balanced_ix = np.concatenate((y_class1_ix, y_class2_ix), axis=0)\\n#    np.random.shuffle(balanced_ix)\\n\\n    # Create X_train dataset (Keras will do the val split)\\n    X_train = X[balanced_ix]\\n    y_train = y[balanced_ix]\\n\\n    if method == \"oversampling\":\\n        X_train = np.reshape(X_train, X_train.shape[:2])\\n        ros = RandomOverSampler(random_state=0)\\n        X_train, y_train = ros.fit_sample(X_train, y_train)\\n        X_train = X_train[..., np.newaxis]\\n    \\n    # Create **UNOFFICIAL** X_val containing only 0s.\\n    balanced_ix_val = np.in1d(range(X.shape[0]), balanced_ix)\\n    \\n    X_val = X[~balanced_ix_val]\\n    y_val = y[~balanced_ix_val]\\n    return X_train, y_train, X_val, y_val\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEPRECATED: Undersample and keep remaining indices for val split\n",
    "\n",
    "\"\"\"\n",
    "def balance_data(X, y, method=\"undersampling\", split=1, **params):\n",
    "    \\\"\"\" Return balanced training dataset obtained by undersampling class 2. \\\"\"\"\n",
    "    y_class1_ix = np.where(y == 1)[0]\n",
    "    y_class2_ix = np.where(y == 0)[0]\n",
    "\n",
    "    # Under-sample class2 to get balanced classes\n",
    "    if method == \"undersampling\":\n",
    "        y_class2_ix = np.random.choice(y_class2_ix, int(len(y_class1_ix) * split), replace=False)\n",
    "    else:\n",
    "        y_class2_ix = np.random.choice(y_class2_ix, int(len(y_class2_ix) * split), replace=False)\n",
    "\n",
    "    # Split train & val\n",
    "    y_class1_ix = np.random.choice(y_class1_ix, int(len(y_class1_ix) * split), replace=False)\n",
    "\n",
    "    # Concatenate the undersampled_class2_array and the class1_array\n",
    "    balanced_ix = np.concatenate((y_class1_ix, y_class2_ix), axis=0)\n",
    "#    np.random.shuffle(balanced_ix)\n",
    "\n",
    "    # Create X_train dataset (Keras will do the val split)\n",
    "    X_train = X[balanced_ix]\n",
    "    y_train = y[balanced_ix]\n",
    "\n",
    "    if method == \"oversampling\":\n",
    "        X_train = np.reshape(X_train, X_train.shape[:2])\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X_train, y_train = ros.fit_sample(X_train, y_train)\n",
    "        X_train = X_train[..., np.newaxis]\n",
    "    \n",
    "    # Create **UNOFFICIAL** X_val containing only 0s.\n",
    "    balanced_ix_val = np.in1d(range(X.shape[0]), balanced_ix)\n",
    "    \n",
    "    X_val = X[~balanced_ix_val]\n",
    "    y_val = y[~balanced_ix_val]\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X, y, method=\"undersampling\", **params):\n",
    "    \"\"\" Return balanced training dataset obtained by undersampling class 2. \"\"\"\n",
    "    if method == \"oversampling\":\n",
    "        sampler = RandomOverSampler(random_state=42)\n",
    "    elif method == \"undersampling\":\n",
    "        sampler = RandomUnderSampler(random_state=42)\n",
    "    else:\n",
    "        raise ValueError('Unrecognized sampling method: ', method)\n",
    "\n",
    "    X, y = sampler.fit_resample(X, y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Extract features:** Use tsfresh to perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP !\n",
    "def extract_tsfresh(X):\n",
    "    # Create a df with integer names for columns so as to facilitate sorting\n",
    "    cols = [col for col in range(1, 50)]\n",
    "    ts_arr = pd.DataFrame(X, columns=cols)\n",
    "    \n",
    "    # Stack the df to conform to tsfresh format\n",
    "    ts_arr = pd.DataFrame(ts_arr.stack()).reset_index()\n",
    "    ts_arr.columns = [\"ID\", \"timestamp\", \"spike_time\"]\n",
    "    fe_arr = extract_features(ts_arr, column_id=\"ID\", column_value=\"spike_time\", column_sort=\"timestamp\")\n",
    "    \n",
    "    # Concatenate into an array, the engineered features df and the time series df\n",
    "    ts_arr = ts_arr.pivot(index='ID', columns='timestamp', values='spike_time')\n",
    "    ts_arr = ts_arr.values\n",
    "    fe_arr = fe_arr.dropna(axis='columns')\n",
    "    fe_arr = fe_arr.values\n",
    "    X = np.concatenate((ts_arr, fe_arr), axis=1)\n",
    "    \n",
    "    # Save extracted features\n",
    "    fname = \"../data/features{}.csv\".format(datetime.datetime.now().strftime(\"%m%d%H%M%S\"))\n",
    "    np.savetxt(fname, X, delimiter=\",\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_features(X):\n",
    "    ts_arr = X[:,:49]\n",
    "    fe_arr = X[:,49:]\n",
    "    return [ts_arr, fe_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Remove features with low variance:** pretty self-explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_var(X, step=\"test\"):\n",
    "    if step == \"train\":        \n",
    "        X = np.concatenate((X[:,:49], sel.fit_transform(X[:,49:])), axis=1)\n",
    "    else:\n",
    "        X = np.concatenate((X[:,:49], sel.transform(X[:,49:])), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Standardization:** Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X, step=\"test\"):\n",
    "    \"\"\"Simple standardization that accepts both a single arr, AND 2 arrays in case of RNN+FeatureEngineering\"\"\"\n",
    "    def standardize(X, step):\n",
    "        if step == \"train\":\n",
    "            return scaler.fit_transform(X)\n",
    "        else:\n",
    "            return scaler.transform(X)\n",
    "\n",
    "    X = np.concatenate((X[:,:49], standardize(X[:,49:], step)), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Get Data:** Main function used to create numpy arrays from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(dfx, dfy, \n",
    "            exclude_neuron_id=True, \n",
    "            balancing=\"undersample\",\n",
    "            standardize=False,\n",
    "            differencing=False,\n",
    "            get_tsfresh=False,\n",
    "            get_ISI_SPIKE=False,\n",
    "            step=\"train\",\n",
    "            RNN=True,\n",
    "            remove_low_variance=True,\n",
    "            split=0.1,\n",
    "            import_tsfresh=True,\n",
    "            **extras):    \n",
    "\n",
    "    y = np.reshape(dfy.values, (dfy.values.shape[0],))\n",
    "    \n",
    "    if not import_tsfresh:\n",
    "        X = dfx.values\n",
    "\n",
    "        # Convert from timeseries to interval\n",
    "        if differencing:\n",
    "            X[:,1:50] -= X[:,:49]\n",
    "            X = np.delete(X, 0, 1)\n",
    " \n",
    "    if import_tsfresh:\n",
    "        X = np.genfromtxt(dfx, delimiter=',')\n",
    "    elif get_tsfresh:\n",
    "        X = extract_tsfresh(X)\n",
    "\n",
    "    print(X.shape)\n",
    "    if get_tsfresh and remove_low_variance:\n",
    "        X = remove_low_var(X, step)\n",
    "        \n",
    "    if standardize:\n",
    "        X = standardize_data(X, step)\n",
    "\n",
    "    if step != \"test\":\n",
    "        X, X_val, y, y_val = train_test_split(X, y, test_size=split, random_state=42)\n",
    "    else:\n",
    "        X_val = np.ones(X.shape)\n",
    "        y_val = np.ones(y.shape)\n",
    "    \n",
    "    if step != \"test\" and balancing:\n",
    "        X, y = balance_data(X, y, method=balancing, **params)\n",
    "    \n",
    "    if params[\"get_tsfresh\"]:\n",
    "        X = isolate_features(X)\n",
    "        if step != \"test\":\n",
    "            X_val = isolate_features(X_val)\n",
    "\n",
    "    if params[\"RNN\"]:\n",
    "        X[0] = X[0][..., np.newaxis]\n",
    "        if step != \"test\":\n",
    "            X_val[0] = X_val[0][..., np.newaxis]\n",
    "            \n",
    "    return X, y, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arxiv: [Neural activity classification with machine learning models trained oninterspike interval series data](https://arxiv.org/pdf/1810.03855.pdf) => PCA and KNN\n",
    "* Github: [PySpike: Python library to analyze spike Train](https://github.com/mariomulansky/PySpike) => Obscure mathematical measurements between spike trains\n",
    "* Profil: [Prof expert en spike train analysis](http://xtof.perso.math.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tensorflow.set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'name': \"LSTM_hybrid\",\n",
    "    'balancing': \"oversampling\",\n",
    "    'standardize': True,\n",
    "    'differencing': True,\n",
    "    'get_tsfresh': True,\n",
    "    'import_tsfresh': True,\n",
    "    'get_ISI_SPIKE': False,\n",
    "    'RNN': True,\n",
    "    'class_weight': False,\n",
    "    'remove_low_variance': True,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 32,\n",
    "    'split': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(params):\n",
    "    global sel, scaler\n",
    "    sel = feature_selection.VarianceThreshold(threshold=.005)\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    \n",
    "    dfx, dfy, dfx_test = import_data()\n",
    "    \n",
    "    if params['import_tsfresh']:\n",
    "        X = \"../data/features0624214715.csv\"\n",
    "        X_test = \"../data/features0624220241.csv\"\n",
    "    else:\n",
    "        X = dfx\n",
    "        X_test = dfx_test\n",
    "\n",
    "    X_train, y_train, X_val, y_val = getData(X, dfy, step=\"train\", **params)\n",
    "    X_test, _, _, _ = getData(X_test, dfy, step=\"test\", **params)\n",
    "    return X_train, y_train, X_val, y_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model):\n",
    "    # Predict on custom X_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.reshape(y_pred, (y_pred.shape[0],))\n",
    "    print (y_pred.shape)\n",
    "    \n",
    "    # Convert sigmoid output to 0s and 1s\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "  \n",
    "    # Format .csv in ENS style\n",
    "    dfy_pred = pd.DataFrame(data=y_pred, columns=[\"TARGET\"], dtype=int)\n",
    "    dfy_pred.index.name = \"ID\"\n",
    "    dfy_pred.index += 16635\n",
    "    return dfy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print(metrics.cohen_kappa_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep-Learning 1: blunt RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16635, 527)\n",
      "(11969, 527)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test = process_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create and train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 324)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           20800       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 49, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 49, 256)      264192      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 256)          525312      lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 288)          0           lstm_4[0][0]                     \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            289         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 812,673\n",
      "Trainable params: 812,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if params['get_tsfresh']:\n",
    "    timestep_nb = X_train[0].shape[1]\n",
    "else:\n",
    "    timestep_nb = X_train.shape[1]\n",
    "spike_per_ts = 1\n",
    "params['cell_nb'] = 256\n",
    "\n",
    "input_tensor = Input(shape=(timestep_nb, spike_per_ts))\n",
    "X = LSTM(params['cell_nb'], return_sequences=True, dropout=params['dropout'])(input_tensor)\n",
    "X = LSTM(params['cell_nb'], return_sequences=False)(X)\n",
    "\n",
    "if params['get_tsfresh']:\n",
    "    additional_features = X_train[1].shape[1]\n",
    "    fe_input = Input(shape=(additional_features,)) # A tensor containing the engineered features\n",
    "    latent = Dense(64, activation='relu')(fe_input)\n",
    "    latent = Dropout(rate=params['dropout'])(latent)\n",
    "    latent = Dense(32, activation='relu')(latent)\n",
    "    latent = Dropout(rate=params['dropout'])(latent)\n",
    "    input_tensor = [input_tensor, fe_input]\n",
    "    X = concatenate([X, latent])   \n",
    "    \n",
    "output_tensor = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25555 samples, validate on 1345 samples\n",
      "Epoch 1/2\n",
      "25555/25555 [==============================] - 347s 14ms/step - loss: 0.2095 - acc: 0.6732 - val_loss: 0.2179 - val_acc: 0.6506\n",
      "Epoch 2/2\n",
      "25555/25555 [==============================] - 343s 13ms/step - loss: 0.1932 - acc: 0.7092 - val_loss: 0.1881 - val_acc: 0.7219\n"
     ]
    }
   ],
   "source": [
    "if params[\"class_weight\"]:\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "else:\n",
    "    class_weights = None\n",
    "if params[\"get_tsfresh\"]:\n",
    "    X_train=list(X_train)\n",
    "model.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=2, validation_split=0.05, class_weight=class_weights, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_params = [\n",
    "    ('batch_size', history.params['batch_size']),\n",
    "    ('epochs', history.params['epochs']),\n",
    "    ('samples', history.params['samples']),\n",
    "    ('val_acc', history.history['val_acc'][-1])\n",
    "    ]\n",
    "\n",
    "params.update(history_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.69      0.78       127\n",
      "           1       0.43      0.75      0.55        40\n",
      "\n",
      "    accuracy                           0.71       167\n",
      "   macro avg       0.67      0.72      0.67       167\n",
      "weighted avg       0.79      0.71      0.73       167\n",
      "\n",
      "0.35480564535204606\n"
     ]
    }
   ],
   "source": [
    "if params[\"get_tsfresh\"]:\n",
    "    X_val=list(X_val)\n",
    "dfy_val = predict(X_val, model)\n",
    "evaluate(y_val, dfy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272, 324)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11969,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16635</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16636</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16637</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16638</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16639</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16640</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16641</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16642</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16643</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16644</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16645</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16646</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16647</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16648</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16652</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16653</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16654</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16655</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16656</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16657</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16658</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16659</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16660</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16661</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16664</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16666</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16669</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TARGET\n",
       "ID           \n",
       "16635       1\n",
       "16636       0\n",
       "16637       0\n",
       "16638       0\n",
       "16639       0\n",
       "16640       0\n",
       "16641       1\n",
       "16642       0\n",
       "16643       0\n",
       "16644       0\n",
       "16645       0\n",
       "16646       0\n",
       "16647       0\n",
       "16648       1\n",
       "16649       0\n",
       "16650       0\n",
       "16651       1\n",
       "16652       0\n",
       "16653       1\n",
       "16654       1\n",
       "16655       0\n",
       "16656       0\n",
       "16657       0\n",
       "16658       0\n",
       "16659       0\n",
       "16660       0\n",
       "16661       0\n",
       "16662       0\n",
       "16663       0\n",
       "16664       0\n",
       "16665       1\n",
       "16666       1\n",
       "16667       1\n",
       "16668       0\n",
       "16669       0"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if params[\"get_tsfresh\"]:\n",
    "    X_test=list(X_test)\n",
    "dfy_pred = predict(X_test, model)\n",
    "dfy_pred[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-knowledge 1: Benchmark = differencing + tsfresh feature engineering + XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nscores = []\\nfor split in np.arange(0.2, 1, 0.1):\\n    params['split'] = split\\n    X_train, y_train, X_val, y_val, X_test = process_data(params)\\n    print(X_train[1].shape)\\n    model.fit(X_train[1], y_train)\\n    dfy_val = predict(X_val[1], model)\\n    scores.append((metrics.f1_score(y_val, dfy_val), metrics.cohen_kappa_score(y_val, dfy_val)))\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "scores = []\n",
    "for split in np.arange(0.2, 1, 0.1):\n",
    "    params['split'] = split\n",
    "    X_train, y_train, X_val, y_val, X_test = process_data(params)\n",
    "    print(X_train[1].shape)\n",
    "    model.fit(X_train[1], y_train)\n",
    "    dfy_val = predict(X_val[1], model)\n",
    "    scores.append((metrics.f1_score(y_val, dfy_val), metrics.cohen_kappa_score(y_val, dfy_val)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16635, 527)\n",
      "(11969, 527)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test = process_data(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['class_weight']:\n",
    "    scale_pos_weight = np.sum(y_train == 0)/ float(np.sum(y_train == 1))\n",
    "else:\n",
    "    scale_pos_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB\n",
    "params_XGB = {\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.5,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'max_depth': 10,\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate': 0.05,\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "        }\n",
    "model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, **params_XGB)\n",
    "\n",
    "#SVC\n",
    "#model = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/goinfre/miniconda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "                   estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                           colsample_bylevel=1,\n",
       "                                           colsample_bytree=0.6, gamma=0.5,\n",
       "                                           learning_rate=0.05, max_delta_step=0,\n",
       "                                           max_depth=10, min_child_weight=1,\n",
       "                                           missing=None, n_estimators=200,\n",
       "                                           n_jobs=1, nthread=None,\n",
       "                                           objective='binary:logistic',\n",
       "                                           random_state=42, reg_alpha=0,\n",
       "                                           reg_lambda=1,\n",
       "                                           scale_pos_weight=4.456593770709079,\n",
       "                                           seed=None, silent=True,\n",
       "                                           subsample=0.8),\n",
       "                   iid='warn', n_iter=1, n_jobs=4,\n",
       "                   param_distributions={'colsample_bytree': [0.6],\n",
       "                                        'gamma': [0.5, 1], 'max_depth': [10],\n",
       "                                        'min_child_weight': [1],\n",
       "                                        'subsample': [0.8]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1001, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=3)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.update(model.get_params())\n",
    "params_XGB = {\n",
    "        'min_child_weight': [1],\n",
    "        'gamma': [0.5, 1],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.6],\n",
    "        'max_depth': [10]\n",
    "        }\n",
    "\"\"\"\n",
    "#search = RandomizedSearchCV(model, param_distributions=params_CV, rchandom_state=42,\n",
    "#                            n_iter=2, cv=3, verbose=1, n_jobs=4, return_train_score=True)\n",
    "\"\"\"\n",
    "search = RandomizedSearchCV(model, param_distributions=params_XGB, n_iter=1, scoring='roc_auc', n_jobs=4, \n",
    "                            verbose=3, random_state=1001 )\n",
    "\n",
    "search.fit(X_train[1], y_train)\n",
    "\n",
    "#model.fit(X_train[1], y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 800, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight='balanced',\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=2,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   iid='warn', n_iter=6, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 266, 333, 400,\n",
       "                                                         466, 533, 600, 666,\n",
       "                                                         733, 800]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 6, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\n",
    "rf_random.fit(X_train[1], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11969,)\n"
     ]
    }
   ],
   "source": [
    "#select_X_test = selection.transform(X_test[1])\n",
    "dfy_pred = predict(X_test[1], best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       127\n",
      "           1       0.55      0.30      0.39        40\n",
      "\n",
      "    accuracy                           0.77       167\n",
      "   macro avg       0.68      0.61      0.62       167\n",
      "weighted avg       0.74      0.77      0.75       167\n",
      "\n",
      "0.2615778450081452\n"
     ]
    }
   ],
   "source": [
    "#select_X_val = selection.transform(X_val[1])\n",
    "dfy_val = predict(X_val[1], best_xgb)\n",
    "evaluate(y_val, dfy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14972, 282)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.bar(range(len(best_xgb.feature_importances_)), best_xgb.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = sorted(best_xgb.feature_importances_)\n",
    "thresh = thresholds[-20]\n",
    "selection = sklearn.feature_selection.SelectFromModel(best_xgb, threshold=thresh, prefit=True)\n",
    "select_X_train = selection.transform(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14971, 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-knowledge 2: KNN with SPIKE- and ISI- synchronization distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../experiments/0625000713'"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def saveExp(dfy_pred, model, params):\n",
    "    \"\"\" Create directory in which to save predictions, experiment parameters and model object. \"\"\"\n",
    "\n",
    "    directory = \"../experiments/{}\".format(datetime.datetime.now().strftime(\"%m%d%H%M%S\"))\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    dfy_pred.to_csv(directory + '/y_pred.csv', sep=',')\n",
    "    \n",
    "    joblib.dump(model, directory + '/model.h5')\n",
    "    \n",
    "    columns = []\n",
    "    values = []\n",
    "    for k, v in params.items():\n",
    "        columns.append(k)\n",
    "        values.append(v)\n",
    "    params_df = pd.DataFrame(data=[values], columns=columns)\n",
    "    params_df.to_csv(directory + '/params.csv', sep=';')\n",
    "    return directory\n",
    "\n",
    "# Save model\n",
    "saveExp(dfy_pred, model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 50, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 382)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 128)          66560       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 32)           12256       input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 160)          0           lstm_11[0][0]                    \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            161         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 78,977\n",
      "Trainable params: 78,977\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(5963, 50, 1)\n",
      "{'': '0', 'name': 'RF_vanilla', 'exclude_neuron_id': True, 'balancing': 'undersample', 'standardize': True, 'differencing': True, 'get_tsfresh': True, 'get_ISI_SPIKE': False, 'RNN': False, 'C': '1.0', 'cache_size': '200', 'class_weight': '', 'coef0': '0.0', 'decision_function_shape': 'ovr', 'degree': '3', 'gamma': 'auto', 'kernel': 'rbf', 'max_iter': '-1', 'probability': False, 'random_state': '', 'shrinking': True, 'tol': '0.001', 'verbose': False}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-10985a3e444f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mload_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-301-10985a3e444f>\u001b[0m in \u001b[0;36mload_exp\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdfy_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-296-20dd024b6af5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X_test, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Predict on custom X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "def load_exp():\n",
    "    \"\"\" Reproduce saved experience from a directory: load dataset, model, predict on x_test and evaluate. \"\"\"\n",
    "    \n",
    "    for xp in os.scandir(\"../experiments\"):\n",
    "    \n",
    "        if not xp.is_dir():\n",
    "            continue\n",
    "        \n",
    "        model_path, params_path, y_pred_path = sorted(os.scandir(xp.path), key=lambda x: (x.is_dir(), x.name))\n",
    "        model = load_model(model_path.path)\n",
    "        model.summary()\n",
    "        \n",
    "        with open(params_path, mode='r') as infile:\n",
    "            reader = csv.reader(infile, delimiter=';')\n",
    "            keys, values = reader\n",
    "        params = {keys[ix]:values[ix] for ix in range(len(keys))}\n",
    "        for k, v in params.items():\n",
    "            if v == 'True' or v == 'False':\n",
    "                params[k] = v == 'True'\n",
    "        X_train, y_train, X_val, y_val, X_test = process_data(params)\n",
    "        if isinstance(X_train, tuple):\n",
    "            print(X_train[0].shape)\n",
    "        else:\n",
    "            print(X_train.shape)\n",
    "        print(params)\n",
    "        \n",
    "        dfy_val = predict(X_val, model)\n",
    "        evaluate(model)\n",
    "        \n",
    "load_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
